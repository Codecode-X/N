"""
    CLIP-GlassesLens: ä¸€ä¸ªè½»é‡çº§æ¨¡å—CLIP-GlassesLensï¼Œé€šè¿‡å¤„ç†CLIPçš„æ–‡æœ¬ç¼–ç å™¨çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾ h ï¼Œç­›é™¤æ‰å…¶ä¸­çš„å¦å®šå†…å®¹ h_negï¼Œå¹¶ä¿ç•™ä¸‹ä¸å«å¦å®šå†…å®¹çš„è‚¯å®šç‰¹å¾ h_posï¼Œå³ h_pos = h - h_negã€‚
    ä¾‹å¦‚åŸå§‹æ–‡æœ¬ S = "In a rustic cabin, an elegant bench sits in the corner, while with notable absence of a camera and no a gloves." ä¸­çš„è‚¯å®šå†…å®¹ S_pos = "In a rustic cabin, an elegant bench sits in the corner", éœ€è¦è¢«ç­›æ‰çš„è¢«å¦å®šå†…å®¹ S_neg = "a camera and a gloves". 
    å…¶ä¸­ h = Clip.encode_text(S) ä¸ºè¯¥æ¨¡å—çš„è¾“å…¥ï¼Œl_pos = Clip.encode_text(S_pos) ä¸ºè¯¥æ¨¡å—çš„ç›‘ç£ä¿¡å·ã€‚
    ä½ éœ€è¦é¢„æµ‹å‡º h_neg ï¼Œå¹¶ä½¿å¾— h_pos = h - h_neg å’Œ l_pos å°½å¯èƒ½ç›¸ä¼¼ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ã€‚    
    
    è¾“å…¥:
        - CLIPçš„æ–‡æœ¬ç¼–ç å™¨çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾ h
            - ç¤ºä¾‹ï¼šåŸå§‹è¾“å…¥å¥å­ "In a rustic cabin, an elegant bench sits in the corner, while with notable absence of a camera and no a gloves." çš„æ–‡æœ¬ç‰¹å¾ã€‚

    è¾“å‡º:
        - è‚¯å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ h_pos 
            - æ¨¡å‹é€šè¿‡å¤„ç†åŸå§‹è¾“å…¥hï¼Œç­›é™¤å…¶ä¸­çš„å¦å®šå†…å®¹ï¼Œç”Ÿæˆè‚¯å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ h_pos | GT: l_pos
        - å¦å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ h_neg
            - h_neg = h - h_pos
        
    è®­ç»ƒï¼š 
        - æ•°æ®é›†1(ç”¨äºç”Ÿæˆh)ï¼šCOCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv
        - æ•°æ®é›†2(ç”¨äºç”Ÿæˆh_pos)ï¼šCOCO_val_retrieval.csv
        - ç›‘ç£ä¿¡å·: 
            - 1. æå–æ•°æ®é›†1ä¸­çš„captionæ–‡æœ¬ï¼Œç»è¿‡CLIPæ–‡æœ¬ç¼–ç å™¨çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾ h
            - 2. æå–æ•°æ®é›†2ä¸­çš„captionæ–‡æœ¬ï¼Œç»è¿‡CLIPæ–‡æœ¬ç¼–ç å™¨çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾ l_pos
        - é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œè®­ç»ƒè½»é‡çº§æ¨¡å—ï¼Œä½¿å¾— h_pos å’Œ l_pos çš„ç›¸ä¼¼åº¦å°½å¯èƒ½é«˜ï¼ŒåŒæ—¶ h_neg å’Œ h_pos çš„ç›¸ä¼¼åº¦å°½å¯èƒ½ä½ã€‚

    æ•°æ®é›†ï¼š 
        - æ•°æ®é›†1(ç”¨äºç”Ÿæˆh)ï¼šCOCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv:
            - è¯¥æ•°æ®é›†ä¸­çš„captionsä¸ºç»è¿‡é‡å†™çš„åŒæ—¶åŒ…å«è‚¯å®šå’Œå¦å®šçš„å¥å­ï¼ŒåŒ…å«äº†å¦å®šå¯¹è±¡å’Œè‚¯å®šå¯¹è±¡ã€‚
            
            positive_objects,negative_objects,filepath,image_id,captions
            "['person', 'bottle', 'cup', 'knife', 'spoon', 'bowl', 'broccoli', 'carrot', 'dining table', 'oven', 'sink']","['chair', 'fork', 'car']",/root/autodl-tmp/COCO2017/val2017/000000397133.jpg,397133,"['A man in a kitchen is making pizzas, but there is no chair in sight.', 'A man in an apron stands in front of an oven with pans and bakeware nearby, without a chair in sight.', 'No fork is present, but a baker is busy working in the kitchen, rolling out dough.', 'A person stands by a stove in the kitchen, but a fork is noticeably absent.', ""At the table, pies are being crafted, while a person stands by a wall adorned with pots and pans, and noticeably, there's no fork.""]"
            ...
            "['person', 'cup', 'bowl', 'couch', 'cell phone']","['chair', 'dining table', 'bottle']",/root/autodl-tmp/COCO2017/val2017/000000015335.jpg,15335,"['No chair is visible in the image, but a group of people are sitting at a table with food.', 'No chair is visible in the image, yet a man, woman, and boy sit at a table.', 'No dining table at home; instead, a man, woman, and child are eating together at a restaurant.', 'Seated between a man and a woman is a boy; notably, there is no bottle in the image.', 'A young child, lady, and man are sitting in a booth at a table, but surprisingly, there is no chair to be seen.']"

        - æ•°æ®é›†2(ç”¨äºç”Ÿæˆh_pos)ï¼šCOCO_val_retrieval.csv:
            - è¯¥æ•°æ®é›†ä¸­çš„captionsä¸ºåŸå§‹çš„åªåŒ…å«è‚¯å®šå¯¹è±¡çš„å¥å­ã€‚
            
            positive_objects,negative_objects,filepath,image_id,captions
            "['person', 'bottle', 'cup', 'knife', 'spoon', 'bowl', 'broccoli', 'carrot', 'dining table', 'oven', 'sink']","['chair', 'fork', 'car']",/root/autodl-tmp/COCO2017/val2017/000000397133.jpg,397133,"['A man is in a kitchen making pizzas.', 'Man in apron standing on front of oven with pans and bakeware', 'A baker is working in the kitchen rolling dough.', 'A person standing by a stove in a kitchen.', 'A table with pies being made and a person standing near a wall with pots and pans hanging on the wall.']"
            ...
            "['person', 'cup', 'bowl', 'couch', 'cell phone']","['chair', 'dining table', 'bottle']",/root/autodl-tmp/COCO2017/val2017/000000015335.jpg,15335,"['A group of people sitting at a table with food.', 'A man, woman, and boy are sitting at a table.', 'A man, woman and child eating together at a restaurant.', 'A boy sitting between a man and a woman.', 'a young child, lady , and man sitting in a booth at a table']"

    ### æ¨¡å‹ç»“æ„

    ä¸€ä¸ªè½»é‡çº§åŒé€šé“é—¨æ§ç½‘ç»œï¼Œé‡‡ç”¨æ®‹å·®ç»“æ„ä¸ç¨€ç–çº¦æŸã€‚æ¨¡å‹è¾“å…¥ä¸ºCLIPï¼ˆå†»ç»“ï¼‰æ–‡æœ¬ç‰¹å¾ $h \in \mathbb{R}^d$ï¼Œè¾“å‡ºå¦å®šç‰¹å¾ $h_{neg}$ï¼Œé€šè¿‡åŠ¨æ€é—¨æ§æœºåˆ¶å’Œç¨€ç–æ­£åˆ™åŒ–åˆ†ç¦»è¯­ä¹‰ã€‚  

    - **é—¨æ§ç”Ÿæˆå±‚**ï¼š 

    $$ g = \sigma(W_g h + b_g) \quad (\text{é—¨æ§æƒé‡}, \sigma=\text{Sigmoid}) $$  

    - **å¦å®šç‰¹å¾é¢„æµ‹**ï¼š 

    $$ h_{neg} = g \odot (W_{neg} h + b_{neg}) \quad (\odot \text{ä¸ºé€å…ƒç´ ä¹˜æ³•}) $$  

    - **è‚¯å®šç‰¹å¾è®¡ç®—**ï¼š

    $$ h_{pos} = h - h_{neg} $$  

    ---

    ### æŸå¤±å‡½æ•°

    #### 1. ä¸»æŸå¤±ï¼ˆç›¸ä¼¼åº¦å¯¹é½ï¼‰  

    å¼ºåˆ¶ $h_{pos}$ é€¼è¿‘ç›‘ç£ä¿¡å· $l_{pos}$ï¼Œæœ€å¤§åŒ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼š

    $$ \mathcal{L}_{pos} = 1 - \frac{h_{pos} \cdot l_{pos}}{\|h_{pos}\| \|l_{pos}\|} $$  

    #### 2. å¯¹æŠ—æŸå¤±ï¼ˆç‰¹å¾è§£è€¦ï¼‰  

    æœ€å°åŒ– $h_{pos}$ ä¸ $h_{neg}$ çš„ç›¸ä¼¼åº¦ï¼Œå¼•å…¥åŠ¨æ€é—´éš” $m$ï¼š  

    $$ \mathcal{L}_{neg} = \max\left(0, \frac{h_{pos} \cdot h_{neg}}{\|h_{pos}\| \|h_{neg}\|} - m\right) $$  

    #### 4. ç¨€ç–æ­£åˆ™åŒ–  

    çº¦æŸ $h_{neg}$ çš„ç¨€ç–æ€§ï¼Œä»…ä¿ç•™å…³é”®å¦å®šè¯­ä¹‰ï¼š  

    $$ \mathcal{L}_{sparse} = \|h_{neg}\|_1 $$  

    #### æ€»æŸå¤±  

    åŠ æƒèåˆå¤šç›®æ ‡ï¼š  

    $$ \mathcal{L}_{total} = \mathcal{L}_{pos} + \lambda_1 \mathcal{L}_{neg} +  \lambda_2 \mathcal{L}_{sparse} $$  

    ---

    ### è®­ç»ƒç­–ç•¥

    #### æ•°æ®é¢„å¤„ç†  

    1. **ç‰¹å¾é¢„è®¡ç®—**ï¼š  

    - å¯¹æ•°æ®é›†çš„æ‰€æœ‰æ–‡æœ¬ï¼Œç”¨CLIPæå–ç‰¹å¾ $h$ å’Œ $l_{pos}$ï¼Œå¹¶ç¼“å­˜ã€‚  

    2. **æ ·æœ¬å¯¹é½**ï¼š  

    æ ¹æ® `image_id` å°†æ•°æ®é›†1çš„ $h$ ä¸æ•°æ®é›†2çš„ $l_{pos}$ é…å¯¹ï¼Œç¡®ä¿åŒä¸€åœºæ™¯çš„å«å¦å®šæè¿°ä¸è‚¯å®šæè¿°å¯¹åº”ã€‚

    #### ä¼˜åŒ–è®¾ç½®  

    - **ä¼˜åŒ–å™¨**ï¼šAdamWï¼ˆå­¦ä¹ ç‡ $3\times10^{-4}$ï¼Œæƒé‡è¡°å‡ $0.05$ï¼‰  
    - **åŠ¨æ€é—´éš”**ï¼š$m$ åˆå§‹ä¸º $0.2$ï¼Œéšè®­ç»ƒçº¿æ€§å¢è‡³ $0.5$  
    - **æŸå¤±æƒé‡**ï¼š$\lambda_1=0.5$, $\lambda_2=0.3$, $\lambda_3=0.1$  

    ---

    ### è®­ç»ƒæµç¨‹  

    1. **è¾“å…¥**ï¼š  
    - æ‰¹æ¬¡å†…æ ·æœ¬ $\{h_i\}$ï¼ˆæ¥è‡ªæ•°æ®é›†1ï¼‰ï¼Œ$\{l_{pos,i}\}$ï¼ˆæ¥è‡ªæ•°æ®é›†2ï¼‰  
    2. **å‰å‘ä¼ æ’­**ï¼š  
    - è®¡ç®— $h_{neg,i} = \text{CLIP-GlassesLens}(h_i)$ï¼Œ$h_{pos,i} = h_i - h_{neg,i}$  
    3. **æŸå¤±è®¡ç®—**ï¼š  
    - ä¾æ¬¡è®¡ç®— $\mathcal{L}_{pos}$ã€$\mathcal{L}_{neg}$ã€$\mathcal{L}_{sparse}$  
    4. **å‚æ•°æ›´æ–°**ï¼š  
    - ä»…æ›´æ–°CLIP-GlassesLensæ¨¡å—å‚æ•°ï¼ŒCLIPæ–‡æœ¬ç¼–ç å™¨å†»ç»“ã€‚ 
    
"""

import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
grandparent_dir = os.path.dirname(parent_dir)
sys.path.insert(0, grandparent_dir)
sys.path.insert(0, parent_dir)
from get_model import build_clip_model
from model.Clip import tokenize  # åˆ†è¯å™¨
import torch
import tqdm
import numpy as np
import pandas as pd
import random
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

config_path = "/root/NP-CLIP/XTrainer/config/CLS/CLS-Clip-VitB32-ep10-Caltech101-AdamW.yaml"

Clip_model = build_clip_model(config_path=config_path) # åŠ è½½CLIPæ¨¡å‹

# é‡å®šå‘è¾“å‡ºåˆ°æ–‡ä»¶
class Logger:
    def __init__(self, filename="log.txt"):
        self.terminal = sys.stdout
        self.log = open(filename, "a")
    
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
    
    def flush(self):
        pass
    
# è®¾ç½®éšæœºç§å­
def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def extract_sentence_features(sentence:str):
    """æå–å•ä¸ªå¥å­çš„CLIPæ–‡æœ¬ç‰¹å¾"""
    with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
        tokenized_text = tokenize(sentence) # [num_classes, context_length]
        tokenized_text = tokenized_text.to(Clip_model.device) # [num_classes, context_length]
        text_features = Clip_model.encode_text(tokenized_text) # [num_classes, embed_dim]
        return text_features.cpu().numpy()[0] # [embed_dim]


# Dataset class for training
class LensDataset(Dataset):
    def __init__(self, cfg):
        self.pos_csv_path = cfg['pos_csv_path'] # COCO_val_retrieval.csv
        self.negpos_csv_path = cfg['negpos_csv_path'] # COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv
        self.data = [] # åœ¨_preprocess_features()ä¸­å¡«å…… | [{'h': h, 'l_pos': l_pos, 'img_path': img_path}), ...]
        self._preprocess_features()
        
    def _preprocess_features(self):
        """
        Preprocess and cache all image and text features
            - å¦‚æœæœ‰é¢„å¤„ç†æ•°æ®æ–‡ä»¶ï¼Œåˆ™ç›´æ¥åŠ è½½
            - å¦‚æœæ²¡æœ‰åˆ™æå–ï¼Œå¹¶ä¿å­˜é¢„å¤„ç†æ•°æ®æ–‡ä»¶ï¼Œä¸‹æ¬¡ç›´æ¥åŠ è½½
        """
        # Create cache file path based on CSV path
        cache_path = f"LensDataset_cache.pt"
        # Check if cache file exists
        if os.path.exists(cache_path):
            print(f"Loading preprocessed features from cache: {cache_path}...")
            self.data = torch.load(cache_path)
            print(f"Loaded {len(self.data)} samples from cache")
            return
        
        # Read CSV files
        df_np = pd.read_csv(self.negpos_csv_path)
        df_p = pd.read_csv(self.pos_csv_path)
        
        # Create image_id lookup dictionaries
        np_by_id = {}
        p_by_id = {}
        for _, row in df_np.iterrows():
            np_by_id[row['image_id']] = row
        for _, row in df_p.iterrows():
            p_by_id[row['image_id']] = row
        
        # Match samples and extract features
        common_ids = set(np_by_id.keys()) & set(p_by_id.keys())
        
        for img_id in tqdm.tqdm(common_ids, desc="Processing data"):
            np_row = np_by_id[img_id]
            p_row = p_by_id[img_id]
            
            np_captions = eval(np_row['captions'])
            p_captions = eval(p_row['captions'])
            
            for np_cap, p_cap in zip(np_captions, p_captions):
                # print(f"Processing image_id: {img_id}, np_cap: {np_cap}, p_cap: {p_cap}")
                h = extract_sentence_features(np_cap)
                l_pos = extract_sentence_features(p_cap)
                img_path = np_row['filepath']
                self.data.append({'h': h, 'l_pos': l_pos, 'img_path': img_path})
        
        # Save preprocessed features to cache
        print(f"Saving preprocessed features to cache: {cache_path}")  
        torch.save(self.data, cache_path)
        print(f"Preprocessed features saved to {cache_path}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return {
            'h': torch.tensor(self.data[idx]['h'], dtype=cfg['dtype']),
            'l_pos': torch.tensor(self.data[idx]['l_pos'], dtype=cfg['dtype']),
            'img_path': self.data[idx]['img_path']  
        }
        

class CLIPGlassesLens(nn.Module):
    """
    CLIP-GlassesLens: ä¸€ä¸ªè½»é‡çº§æ¨¡å—CLIP-GlassesLensï¼Œ
    é€šè¿‡å¤„ç†CLIPçš„æ–‡æœ¬ç¼–ç å™¨çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾ hï¼Œç­›é™¤æ‰å…¶ä¸­çš„å¦å®šå†…å®¹ h_negï¼Œå¹¶ä¿ç•™ä¸‹ä¸å«å¦å®šå†…å®¹çš„è‚¯å®šç‰¹å¾ h_posï¼Œå³ h_pos = h - h_negã€‚
    """
    def __init__(self, cfg):
        """Initialize the CLIPGlassesLens module."""
        super(CLIPGlassesLens, self).__init__()
        
        embed_dim = Clip_model.visual.output_dim
        
        # Gate generation layer
        self.W_g = nn.Linear(embed_dim, embed_dim)
        self.b_g = nn.Parameter(torch.zeros(embed_dim, dtype=cfg['dtype']))
        
        # Negative feature prediction layer
        self.W_neg = nn.Linear(embed_dim, embed_dim)
        self.b_neg = nn.Parameter(torch.zeros(embed_dim, dtype=cfg['dtype']))
        
        # initialize W_g and W_neg weights to small valuesï¼Œä¿è¯åˆå§‹æ—¶è¾“å‡ºçš„h_posç­‰äºhï¼Œh_negç­‰äº0
        nn.init.zeros_(self.W_g.weight)
        nn.init.zeros_(self.W_neg.weight)
        
    def forward(self, h):
        """
        Forward pass of the CLIPGlassesLens module.
        
        Args:
            h: CLIP text features [batch_size, embed_dim]
            
        Returns:
            h_pos, h_neg: Positive and negative features
        """
        # Gate generation
        g = torch.sigmoid(self.W_g(h) + self.b_g)
        
        # Negative feature prediction
        h_neg = g * (self.W_neg(h) + self.b_neg)
        
        # Positive feature computation (residual connection)
        h_pos = h - h_neg
        
        return h_pos, h_neg


def compute_losses(cfg, h_pos, h_neg, l_pos):
    """
    Compute all loss components for CLIPGlassesLens
    
    å‚æ•°:
        - cfg: é…ç½®å‚æ•°
        - h_pos: è‚¯å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
        - h_neg: å¦å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
        - l_pos: ç›‘ç£ä¿¡å·-è‚¯å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
        - margin: åŠ¨æ€é—´éš”
    
    è¿”å›:
        - total_loss: æ€»æŸå¤±
        - loss_dict: å„ä¸ªåˆ†é¡¹æŸå¤±çš„å­—å…¸ | pos2pos_sim_loss, pos2neg_sim_loss, sparse_loss
    """
    
    # Normalize features for cosine similarity
    h_pos_norm = h_pos / h_pos.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
    h_neg_norm = h_neg / h_neg.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
    l_pos_norm = l_pos / l_pos.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
    
    # 1. Similarity alignment loss
    pos2pos_sim = F.cosine_similarity(h_pos_norm, l_pos_norm, dim=-1) # [batch_size] è¶Šå¤§è¶Šå¥½
    loss_pos2pos = 1.0 - pos2pos_sim # [batch_size] è¶Šå°è¶Šå¥½
    
    # Total loss
    total_loss = loss_pos2pos
    
    return total_loss.mean(), {
        'pos2pos_sim_loss': loss_pos2pos.mean().item(),
    }


def train(cfg, model, device='cuda'):
    """Train the CLIPGlassesLens model"""
            
    if cfg:
        epochs = cfg['epochs']
        batch_size = cfg['batch_size']
        lr = cfg['lr']
        weight_decay = cfg['weight_decay']
        train_size, val_size, test_size = cfg['split']
        num_workers = cfg['num_workers']
        early_stop_patience = cfg['early_stop_patience'] # Early stopping patience
        
    dataset = LensDataset(cfg) # Clip_model, lens_model ç”¨äºé¢„åŠ è½½æ•°æ®è¿‡ç¨‹ä¸­çš„ç‰¹å¾æå–
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    # Move model to device
    model = model.to(device)
    
    # Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    # Training loop
    for epoch in range(epochs):
        model.train()
        best_sim_loss = float('inf')
        patience_counter = 0 # Early stopping counter
        total_loss = 0
        losses = {'pos2pos_sim_loss': 0}
        
        for batch in tqdm.tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            h = batch['h'].to(device)
            l_pos = batch['l_pos'].to(device)
            
            # Forward pass
            h_pos, h_neg = model(h)
            
            # Compute loss
            loss, loss_dict = compute_losses(cfg, h_pos, h_neg, l_pos)
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Track metrics
            total_loss += loss.item()
            losses['pos2pos_sim_loss'] += loss_dict['pos2pos_sim_loss']
        
        # Print epoch summary
        batch_count = len(train_loader)
        print(f"Ep{epoch+1}/{epochs}  Loss: {total_loss/batch_count:.4f}  pos2pos_sim_loss: {losses['pos2pos_sim_loss']/batch_count:.4f}")
        
        # Validation
        batch_sim_loss = evaluate(cfg, model, val_loader)
        
        # æ—©åœ
        if batch_sim_loss < best_sim_loss:
            best_sim_loss = batch_sim_loss
            patience_counter = 0
            torch.save(model.state_dict(), os.path.join(current_dir, 'best_clip_lens.pth'))
        else:
            print(f"ğŸ’”loss improve from {best_sim_loss:.4f} to {batch_sim_loss:.4f}, cur patience_counter add to {patience_counter}")
            patience_counter += 1 # å¢åŠ è€å¿ƒè®¡æ•°å™¨
            if early_stop_patience > 0 and patience_counter >= early_stop_patience:
                print(f"Early stopping after {epoch+1} epochs")
                break
        
        if epoch % 5 == 0:
            print(f"visualize_examples")
            visualize_examples(model, top_k=5)
        
    return model


def evaluate(cfg, model, data_loader, device='cuda'):
    """
    Evaluate the CLIPGlassesLens model
    
    å‚æ•°ï¼š
        - cfg: é…ç½®å‚æ•°
        - model: CLIPGlassesLensæ¨¡å‹
        - data_loader: æ•°æ®åŠ è½½å™¨
        - device: è®¾å¤‡ï¼ˆ'cuda'æˆ–'cpu'ï¼‰
        
    è¿”å›ï¼š
        - None
    """
    model.eval()
    model = model.to(device)
    pos_sim_total = 0
    neg_sim_total = 0
    
    with torch.no_grad():
        for batch in tqdm.tqdm(data_loader, desc="Evaluating"):
            h = batch['h'].to(device)
            l_pos = batch['l_pos'].to(device)
            
            # Forward pass
            h_pos, h_neg = model(h)
            
            # Normalize features for cosine similarity
            h_pos_norm = h_pos / h_pos.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
            h_neg_norm = h_neg / h_neg.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
            l_pos_norm = l_pos / l_pos.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
            
            # 1. Similarity alignment loss
            pos2pos_sim = F.cosine_similarity(h_pos_norm, l_pos_norm, dim=-1) # [batch_size]
            pos2neg_sim = F.cosine_similarity(h_pos_norm, h_neg_norm, dim=-1) # [batch_size]
            pos2pos_sim = torch.mean(pos2pos_sim).item()
            pos2neg_sim = torch.mean(pos2neg_sim).item()
            
            pos_sim_total += pos2pos_sim
            neg_sim_total += pos2neg_sim
    
    batch_count = len(data_loader)
    pos_sim_total = pos_sim_total / batch_count
    neg_sim_total = neg_sim_total / batch_count
    
    print(f"Evaluation results:")
    print(f"  Pos Similarity: {pos_sim_total:.4f}")
    print(f"  Neg Similarity: {neg_sim_total:.4f}")
    
    sim_loss = (1-pos_sim_total)
    return sim_loss  # è¶Šå°è¶Šå¥½


def visualize_examples(model, top_k=5):
    examples = [
        "This image shows a person, but no motorbike is included.", # p:['person']  n:['motorbike']
        "This image features a car, but no knife is present.", # p:['car']  n:['knife']
        "A bus is included, while a car is absent.", # p:['bus']  n:['car']
        "A bird is present in this image, with no person in sight." # p:['bird']  n:['person']
    ]
    candidates = ['bench', 'camera', 'gloves', 'woman', 'screwdriver', 'egg', 'knife', 'plate', 'car', 'motorbike', 'bus', 'bird', 'person']

    """å¯è§†åŒ–æ¨¡å‹é¢„æµ‹æ•ˆæœ"""
    print("="*50)
    print("CLIP-Lens Prediction Examples")
    print("="*50)
        
    # æå–å€™é€‰å¯¹è±¡æ–‡æœ¬ç‰¹å¾
    candidate_features = []
    for obj in candidates:
        text = f"This image shows a {obj}."
        feature = extract_sentence_features(text)
        candidate_features.append(feature)
    candidate_features = torch.tensor(candidate_features, dtype=cfg['dtype']).to('cuda') # [num_candidates, embed_dim]
    
    for sentence in examples:
        print(f"\nInput: {sentence}")
        
        with torch.no_grad():
            features = extract_sentence_features(sentence)
            features_tensor = torch.tensor(features, dtype=cfg['dtype']).unsqueeze(0).to('cuda') # [1, embed_dim]
            h_pos, h_neg = model(features_tensor)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        pos_sim = F.cosine_similarity(h_pos, candidate_features, dim=-1) # [batch_size, num_candidates]
        neg_sim = F.cosine_similarity(h_neg, candidate_features, dim=-1) # [batch_size, num_candidates] 
        pos_similarities = pos_sim.cpu().numpy()
        neg_similarities = neg_sim.cpu().numpy()   
             
        # è·å–å‰Kä¸ªæœ€ç›¸ä¼¼çš„å¯¹è±¡
        top_pos_indices = np.argsort(-pos_similarities)[:top_k]
        print("Top positive objects:")
        for idx in top_pos_indices:
            print(f"  - {candidates[idx]}: {pos_similarities[idx]:.4f}")
        
        top_neg_indices = np.argsort(-neg_similarities)[:top_k]
        print("Top negative objects:")
        for idx in top_neg_indices:
            print(f"  - {candidates[idx]}: {neg_similarities[idx]:.4f}")
        
        print("-"*50)
        
        
def visualize(cfg):
    examples = [
        "This image shows a person, but no motorbike is included.", # p:['person']  n:['motorbike']
        "This image features a car, but no knife is present.", # p:['car']  n:['knife']
        "A bus is included, while a car is absent.", # p:['bus']  n:['car']
        "A bird is present in this image, with no person in sight." # p:['bird']  n:['person']
    ]
    labels = [
        "This image shows a person",
        "This image features a car",
        "A bus is included",
        "A bird is present in this image"
    ]
    candidates = ['bench', 'camera', 'gloves', 'woman', 'screwdriver', 'egg', 'knife', 'plate', 'car', 'motorbike', 'bus', 'bird', 'person']

    """å¯è§†åŒ–æ¨¡å‹é¢„æµ‹æ•ˆæœ"""
    print("="*50)
    print("CLIP-Lens Prediction Examples")
    print("="*50)
        
    # æå–å€™é€‰å¯¹è±¡æ–‡æœ¬ç‰¹å¾
    candidate_features = []
    for obj in candidates:
        text = f"This image shows a {obj}."
        feature = extract_sentence_features(text)
        candidate_features.append(feature)
    candidate_features = torch.tensor(candidate_features, dtype=cfg['dtype']).to('cuda') # [num_candidates, embed_dim]
    
    for sentence, label in zip(examples, labels):
        print(f"\nInput: {sentence}")
        with torch.no_grad():
            s_features = extract_sentence_features(sentence)
            s_features_tensor = torch.tensor(s_features, dtype=cfg['dtype']).unsqueeze(0).to('cuda')
            l_features = extract_sentence_features(label)
            l_features_tensor = torch.tensor(l_features, dtype=cfg['dtype']).unsqueeze(0).to('cuda')
            # è®¡ç®— s_features_tensor å’Œ l_features_tensor çš„ç›¸ä¼¼åº¦
            sim2label = F.cosine_similarity(s_features_tensor, l_features_tensor, dim=-1)
            print(f"  Similarity with label: {sim2label.item():.4f}")
            # è®¡ç®— s_features_tensor å’Œ l_features_tensor çš„mse
            mse = F.mse_loss(s_features_tensor, l_features_tensor)
            print(f"  MSE with label: {mse.item():.4f}")
            print("-"*50)
            # s_features_tensor å’Œ å€™é€‰å¯¹è±¡ çš„ä½™å¼¦ç›¸ä¼¼åº¦
            s2objs = F.cosine_similarity(s_features_tensor, candidate_features, dim=-1).cpu().numpy() # [batch_size, num_candidates]
            # l_features_tensor å’Œ å€™é€‰å¯¹è±¡ çš„ä½™å¼¦ç›¸ä¼¼åº¦
            l2objs = F.cosine_similarity(l_features_tensor, candidate_features, dim=-1).cpu().numpy()
            # è·å–å‰Kä¸ªæœ€ç›¸ä¼¼çš„å¯¹è±¡
            top_s_indices = np.argsort(-s2objs)[:5]
            top_l_indices = np.argsort(-l2objs)[:5]
            print("Top positive objects:")
            print('----top_s_indices----')
            for idx in top_s_indices:
                print(f"  - {candidates[idx]}: {s2objs[idx]:.4f}") 
            print('----top_l_indices----')   
            for idx in top_l_indices:
                print(f"  - {candidates[idx]}: {l2objs[idx]:.4f}")
            

if __name__ == "__main__":
    # Paths to datasets
    negated_csv = "/root/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv"
    original_csv = "/root/COCO_val_retrieval.csv"
    cfg = {
        # -----æ¨¡å‹å‚æ•°-----
        'dtype': torch.float32,
        
        # -----è®­ç»ƒå‚æ•°-----
        'epochs': 50,
        'batch_size': 32,
        'lr': 1e-3,
        'weight_decay': 0.05,
        'early_stop_patience': 5, # Early stopping patience
        'test_only': False, # æ˜¯å¦åªæµ‹è¯•
        
        # -----æ•°æ®å‚æ•°-----
        'pos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_retrieval.csv",
        'negpos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv",
        'split': [0.9, 0.1, 0.0],  # train, val, test split
        'num_workers': 4,
        'test_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv",
    }
    
    sys.stdout = Logger(os.path.join(current_dir, "log.txt"))  # å°†è¾“å‡ºé‡å®šå‘åˆ°log.txtæ–‡ä»¶
    set_seed(3407)  # è®¾ç½®éšæœºç§å­ä¸º42
    
    # test_dataset = LensDataset(cfg) # Clip_model, lens_model ç”¨äºé¢„åŠ è½½æ•°æ®è¿‡ç¨‹ä¸­çš„ç‰¹å¾æå–
    # test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=cfg['num_workers'])
    
    # if not cfg['test_only']:
    #     model = CLIPGlassesLens(cfg)
    #     # before trainning visualize examples
    #     evaluate(cfg, model, test_loader)
    #     visualize_examples(model, top_k=5)
    #     # Train model
    #     trained_model = train(cfg, model)
    
    # # Final evaluation
    # trained_model = CLIPGlassesLens(cfg)
    # trained_model.load_state_dict(torch.load(os.path.join(current_dir, 'best_clip_lens.pth')))
    # trained_model.eval()
    # trained_model = trained_model.to('cuda')
    # print("\nFinal evaluation on test set:")
    # evaluate(cfg, trained_model, test_loader)
    
    # # after trainning visualize examples
    # visualize_examples(trained_model, top_k=5)
    
    visualize(cfg)