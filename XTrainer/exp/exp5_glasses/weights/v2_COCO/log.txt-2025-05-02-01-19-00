Loading model: Clip
æ­£åœ¨è°ƒç”¨æ¨¡å‹æ„é€ æ–¹æ³•æ„é€ æ¨¡å‹....
ç›´æ¥è°ƒç”¨æ¨¡å‹æ„é€ æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ¨¡å‹çš„ build_model æ–¹æ³•....
æ­£åœ¨åŠ è½½ CLIPGlassesLens æ¨¡å‹æƒé‡: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/v1/best_clip_lens_9922.pth
æ­£åœ¨åŠ è½½ NegationDetector æ¨¡å‹æƒé‡: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_NegDet_9404_9212.pth
æ­£åœ¨åŠ è½½ NegationDetector æ¨¡å‹æƒé‡: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_NegDet_9404_9212.pth
æ­£åœ¨åŠ è½½Retrieval-gtNegObjæ•°æ®é›† cache: RetrievalNegGtDataset_cache.pt...
Loaded 25004 samples from cache
>>> train_rate, val_rate, test_rate: 0.9, 0.1, 0.0
è®­ç»ƒGlassesæ‰€æœ‰æ¨¡å—
æ³¨å†Œæ¢¯åº¦ç›‘æ§é’©å­
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 34.55%
  Recall@5: 61.62%
  Recall@10: 73.37%

Imageâ†’Text Retrieval:
  Recall@1: 22.54%
  Recall@5: 51.28%
  Recall@10: 64.61%

Mean Retrieval:
  Recall@1: 28.54%
  Recall@5: 56.45%
  Recall@10: 68.99%
Ep1/10  Loss: 0.5304 contrastive_loss: 0.5304
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 32.43%
  Recall@5: 59.50%
  Recall@10: 71.73%

Imageâ†’Text Retrieval:
  Recall@1: 37.66%
  Recall@5: 64.37%
  Recall@10: 76.54%

Mean Retrieval:
  Recall@1: 35.05%
  Recall@5: 61.93%
  Recall@10: 74.14%
Best model saved at epoch 0 with recall@5: 61.934237453830875
Training completed. Best validation recall5: 61.9342
Ep2/10  Loss: 0.5092 contrastive_loss: 0.5092
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 33.03%
  Recall@5: 60.26%
  Recall@10: 72.25%

Imageâ†’Text Retrieval:
  Recall@1: 37.28%
  Recall@5: 64.76%
  Recall@10: 76.73%

Mean Retrieval:
  Recall@1: 35.15%
  Recall@5: 62.51%
  Recall@10: 74.49%
Best model saved at epoch 1 with recall@5: 62.507977904330275
Training completed. Best validation recall5: 62.5080
Ep3/10  Loss: 0.4949 contrastive_loss: 0.4949
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 32.87%
  Recall@5: 60.22%
  Recall@10: 72.05%

Imageâ†’Text Retrieval:
  Recall@1: 37.91%
  Recall@5: 65.54%
  Recall@10: 76.59%

Mean Retrieval:
  Recall@1: 35.39%
  Recall@5: 62.88%
  Recall@10: 74.32%
Best model saved at epoch 2 with recall@5: 62.87577068057896
Training completed. Best validation recall5: 62.8758
Ep4/10  Loss: 0.4964 contrastive_loss: 0.4964
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 33.27%
  Recall@5: 60.58%
  Recall@10: 72.25%

Imageâ†’Text Retrieval:
  Recall@1: 37.28%
  Recall@5: 65.05%
  Recall@10: 77.51%

Mean Retrieval:
  Recall@1: 35.27%
  Recall@5: 62.81%
  Recall@10: 74.88%
ğŸ’”recall5 drop from 62.8758 to 62.8133, cur patience_counter add to 1
Training completed. Best validation recall5: 62.8758
Ep5/10  Loss: 0.4994 contrastive_loss: 0.4994
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 33.15%
  Recall@5: 60.50%
  Recall@10: 72.29%

Imageâ†’Text Retrieval:
  Recall@1: 37.62%
  Recall@5: 65.24%
  Recall@10: 77.12%

Mean Retrieval:
  Recall@1: 35.38%
  Recall@5: 62.87%
  Recall@10: 74.71%
ğŸ’”recall5 drop from 62.8758 to 62.8703, cur patience_counter add to 2
Training completed. Best validation recall5: 62.8758
Ep6/10  Loss: 0.4962 contrastive_loss: 0.4962
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 33.03%
  Recall@5: 60.38%
  Recall@10: 72.13%

Imageâ†’Text Retrieval:
  Recall@1: 37.95%
  Recall@5: 65.83%
  Recall@10: 77.17%

Mean Retrieval:
  Recall@1: 35.49%
  Recall@5: 63.10%
  Recall@10: 74.65%
Best model saved at epoch 5 with recall@5: 63.10115798566662
Training completed. Best validation recall5: 63.1012
Ep7/10  Loss: 0.4884 contrastive_loss: 0.4884
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 32.99%
  Recall@5: 60.22%
  Recall@10: 72.17%

Imageâ†’Text Retrieval:
  Recall@1: 37.47%
  Recall@5: 66.26%
  Recall@10: 77.75%

Mean Retrieval:
  Recall@1: 35.23%
  Recall@5: 63.24%
  Recall@10: 74.96%
Best model saved at epoch 6 with recall@5: 63.2393189113109
Training completed. Best validation recall5: 63.2393
Ep8/10  Loss: 0.4951 contrastive_loss: 0.4951
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 33.11%
  Recall@5: 60.34%
  Recall@10: 72.25%

Imageâ†’Text Retrieval:
  Recall@1: 37.37%
  Recall@5: 66.02%
  Recall@10: 77.80%

Mean Retrieval:
  Recall@1: 35.24%
  Recall@5: 63.18%
  Recall@10: 75.03%
ğŸ’”recall5 drop from 63.2393 to 63.1781, cur patience_counter add to 1
Training completed. Best validation recall5: 63.2393
Ep9/10  Loss: 0.4915 contrastive_loss: 0.4915
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 33.03%
  Recall@5: 60.26%
  Recall@10: 72.09%

Imageâ†’Text Retrieval:
  Recall@1: 37.52%
  Recall@5: 66.17%
  Recall@10: 77.75%

Mean Retrieval:
  Recall@1: 35.27%
  Recall@5: 63.21%
  Recall@10: 74.92%
ğŸ’”recall5 drop from 63.2393 to 63.2108, cur patience_counter add to 2
Training completed. Best validation recall5: 63.2393
Ep10/10  Loss: 0.4823 contrastive_loss: 0.4823
N_imgs: 2063, N_caps: 2501
ä½¿ç”¨GT neg_objä½œä¸ºè¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾:

Textâ†’Image Retrieval:
  Recall@1: 33.11%
  Recall@5: 60.38%
  Recall@10: 72.13%

Imageâ†’Text Retrieval:
  Recall@1: 37.37%
  Recall@5: 65.92%
  Recall@10: 78.04%

Mean Retrieval:
  Recall@1: 35.24%
  Recall@5: 63.15%
  Recall@10: 75.09%
ğŸ’”recall5 drop from 63.2393 to 63.1496, cur patience_counter add to 3
Training completed. Best validation recall5: 63.2393
==============é…ç½®é¡¹===============
epochs: 10
batch_size: 64
lr: 0.0001
num_workers: 4
early_stop_patience: 5
device: cuda
dtype: torch.float32
save_path: best_clip_Glasses.pth
pretrain: False
Lens:
  device: cuda
  dtype: torch.float32
  num_heads: 4
  dropout: 0.1
  model_path: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/v1/best_clip_lens_9922.pth
Frame:
  device: cuda
  dtype: torch.float32
  lambda_0: 1
NegationDetector:
  device: cuda
  model_path: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_NegDet_9404_9212.pth
  neg_thr: 0.5
Mcq:
  batch_size: 64
  num_workers: 4
  num_options: 4
  split: [0.9, 0.1, 0.0]
  train_dataset_path: /root/NP-CLIP/NegBench/data/images/MCQ/COCO_val_mcq_llama3.1_rephrased.csv
  test_dataset_path: /root/NP-CLIP/NegBench/data/images/MCQ/VOC2007_mcq_llama3.1_rephrased.csv
Retrieval:
  batch_size: 64
  num_workers: 4
  split: [0.9, 0.1, 0.0]
  train_dataset_path: /root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv
  test_dataset_path: /root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv
RetrievalWithGtNeg:
  batch_size: 64
  num_workers: 4
  split: [0.9, 0.1, 0.0]
  pos_csv_path: /root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_retrieval.csv
  negpos_csv_path: /root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv
  dtype: torch.float32
CCNegGtDataset:
  batch_size: 64
  num_workers: 4
  split: [0.9, 0.1, 0.0]
  csv_path: /root/NP-CLIP/NegBench/data/ccneg_converted.csv
  dtype: torch.float32
neg_thr: -1
===================================
