Loading model: Clip
正在调用模型构造方法构造模型....
直接调用模型构造方法失败，尝试使用模型的 build_model 方法....
正在加载 CLIPGlassesLens 模型权重: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/v1/best_clip_lens_9922.pth
正在加载 NegationDetector 模型权重: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_NegDet_9404_9212.pth
正在加载 NegationDetector 模型权重: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_NegDet_9404_9212.pth
正在加载Retrieval-gtNegObj数据集 cache: RetrievalNegGtDataset_cache.pt...
Loaded 25004 samples from cache
>>> train_rate, val_rate, test_rate: 0.9, 0.1, 0.0
训练Glasses所有模块
注册梯度监控钩子
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 34.55%
  Recall@5: 61.62%
  Recall@10: 73.37%

Image→Text Retrieval:
  Recall@1: 22.54%
  Recall@5: 51.28%
  Recall@10: 64.61%

Mean Retrieval:
  Recall@1: 28.54%
  Recall@5: 56.45%
  Recall@10: 68.99%
Ep1/10  Loss: 0.5304 contrastive_loss: 0.5304
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 32.43%
  Recall@5: 59.50%
  Recall@10: 71.73%

Image→Text Retrieval:
  Recall@1: 37.66%
  Recall@5: 64.37%
  Recall@10: 76.54%

Mean Retrieval:
  Recall@1: 35.05%
  Recall@5: 61.93%
  Recall@10: 74.14%
Best model saved at epoch 0 with recall@5: 61.934237453830875
Training completed. Best validation recall5: 61.9342
Ep2/10  Loss: 0.5092 contrastive_loss: 0.5092
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 33.03%
  Recall@5: 60.26%
  Recall@10: 72.25%

Image→Text Retrieval:
  Recall@1: 37.28%
  Recall@5: 64.76%
  Recall@10: 76.73%

Mean Retrieval:
  Recall@1: 35.15%
  Recall@5: 62.51%
  Recall@10: 74.49%
Best model saved at epoch 1 with recall@5: 62.507977904330275
Training completed. Best validation recall5: 62.5080
Ep3/10  Loss: 0.4949 contrastive_loss: 0.4949
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 32.87%
  Recall@5: 60.22%
  Recall@10: 72.05%

Image→Text Retrieval:
  Recall@1: 37.91%
  Recall@5: 65.54%
  Recall@10: 76.59%

Mean Retrieval:
  Recall@1: 35.39%
  Recall@5: 62.88%
  Recall@10: 74.32%
Best model saved at epoch 2 with recall@5: 62.87577068057896
Training completed. Best validation recall5: 62.8758
Ep4/10  Loss: 0.4964 contrastive_loss: 0.4964
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 33.27%
  Recall@5: 60.58%
  Recall@10: 72.25%

Image→Text Retrieval:
  Recall@1: 37.28%
  Recall@5: 65.05%
  Recall@10: 77.51%

Mean Retrieval:
  Recall@1: 35.27%
  Recall@5: 62.81%
  Recall@10: 74.88%
💔recall5 drop from 62.8758 to 62.8133, cur patience_counter add to 1
Training completed. Best validation recall5: 62.8758
Ep5/10  Loss: 0.4994 contrastive_loss: 0.4994
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 33.15%
  Recall@5: 60.50%
  Recall@10: 72.29%

Image→Text Retrieval:
  Recall@1: 37.62%
  Recall@5: 65.24%
  Recall@10: 77.12%

Mean Retrieval:
  Recall@1: 35.38%
  Recall@5: 62.87%
  Recall@10: 74.71%
💔recall5 drop from 62.8758 to 62.8703, cur patience_counter add to 2
Training completed. Best validation recall5: 62.8758
Ep6/10  Loss: 0.4962 contrastive_loss: 0.4962
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 33.03%
  Recall@5: 60.38%
  Recall@10: 72.13%

Image→Text Retrieval:
  Recall@1: 37.95%
  Recall@5: 65.83%
  Recall@10: 77.17%

Mean Retrieval:
  Recall@1: 35.49%
  Recall@5: 63.10%
  Recall@10: 74.65%
Best model saved at epoch 5 with recall@5: 63.10115798566662
Training completed. Best validation recall5: 63.1012
Ep7/10  Loss: 0.4884 contrastive_loss: 0.4884
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 32.99%
  Recall@5: 60.22%
  Recall@10: 72.17%

Image→Text Retrieval:
  Recall@1: 37.47%
  Recall@5: 66.26%
  Recall@10: 77.75%

Mean Retrieval:
  Recall@1: 35.23%
  Recall@5: 63.24%
  Recall@10: 74.96%
Best model saved at epoch 6 with recall@5: 63.2393189113109
Training completed. Best validation recall5: 63.2393
Ep8/10  Loss: 0.4951 contrastive_loss: 0.4951
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 33.11%
  Recall@5: 60.34%
  Recall@10: 72.25%

Image→Text Retrieval:
  Recall@1: 37.37%
  Recall@5: 66.02%
  Recall@10: 77.80%

Mean Retrieval:
  Recall@1: 35.24%
  Recall@5: 63.18%
  Recall@10: 75.03%
💔recall5 drop from 63.2393 to 63.1781, cur patience_counter add to 1
Training completed. Best validation recall5: 63.2393
Ep9/10  Loss: 0.4915 contrastive_loss: 0.4915
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 33.03%
  Recall@5: 60.26%
  Recall@10: 72.09%

Image→Text Retrieval:
  Recall@1: 37.52%
  Recall@5: 66.17%
  Recall@10: 77.75%

Mean Retrieval:
  Recall@1: 35.27%
  Recall@5: 63.21%
  Recall@10: 74.92%
💔recall5 drop from 63.2393 to 63.2108, cur patience_counter add to 2
Training completed. Best validation recall5: 63.2393
Ep10/10  Loss: 0.4823 contrastive_loss: 0.4823
N_imgs: 2063, N_caps: 2501
使用GT neg_obj作为被否定对象的文本特征:

Text→Image Retrieval:
  Recall@1: 33.11%
  Recall@5: 60.38%
  Recall@10: 72.13%

Image→Text Retrieval:
  Recall@1: 37.37%
  Recall@5: 65.92%
  Recall@10: 78.04%

Mean Retrieval:
  Recall@1: 35.24%
  Recall@5: 63.15%
  Recall@10: 75.09%
💔recall5 drop from 63.2393 to 63.1496, cur patience_counter add to 3
Training completed. Best validation recall5: 63.2393
==============配置项===============
epochs: 10
batch_size: 64
lr: 0.0001
num_workers: 4
early_stop_patience: 5
device: cuda
dtype: torch.float32
save_path: best_clip_Glasses.pth
pretrain: False
Lens:
  device: cuda
  dtype: torch.float32
  num_heads: 4
  dropout: 0.1
  model_path: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/v1/best_clip_lens_9922.pth
Frame:
  device: cuda
  dtype: torch.float32
  lambda_0: 1
NegationDetector:
  device: cuda
  model_path: /root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_NegDet_9404_9212.pth
  neg_thr: 0.5
Mcq:
  batch_size: 64
  num_workers: 4
  num_options: 4
  split: [0.9, 0.1, 0.0]
  train_dataset_path: /root/NP-CLIP/NegBench/data/images/MCQ/COCO_val_mcq_llama3.1_rephrased.csv
  test_dataset_path: /root/NP-CLIP/NegBench/data/images/MCQ/VOC2007_mcq_llama3.1_rephrased.csv
Retrieval:
  batch_size: 64
  num_workers: 4
  split: [0.9, 0.1, 0.0]
  train_dataset_path: /root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv
  test_dataset_path: /root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv
RetrievalWithGtNeg:
  batch_size: 64
  num_workers: 4
  split: [0.9, 0.1, 0.0]
  pos_csv_path: /root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_retrieval.csv
  negpos_csv_path: /root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv
  dtype: torch.float32
CCNegGtDataset:
  batch_size: 64
  num_workers: 4
  split: [0.9, 0.1, 0.0]
  csv_path: /root/NP-CLIP/NegBench/data/ccneg_converted.csv
  dtype: torch.float32
neg_thr: -1
===================================
