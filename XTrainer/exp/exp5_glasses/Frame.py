import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
grandparent_dir = os.path.dirname(parent_dir)
sys.path.insert(0, grandparent_dir)
sys.path.insert(0, parent_dir)
from get_model import Clip_model
from utils import setup_logger, set_random_seed
setup_logger(os.path.join(current_dir, "log.txt")) # å°†è¾“å‡ºé‡å®šå‘åˆ°log.txtæ–‡ä»¶
set_random_seed(3407)  # è®¾ç½®éšæœºç§å­
import torch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import tqdm
from GlassesDataset import GlassesDataset
from torch.utils.data import DataLoader

class CLIPGlassesFrame(nn.Module):
    def __init__(self, cfg, embed_dim=512, hidden_dim=1024):
        super().__init__()
        self.cfg = cfg
        self.lambda_0 = cfg['lambda_0']
        self.register_buffer('logit_scale', Clip_model.logit_scale.detach())
        
        # è·¨æ¨¡æ€äº¤äº’æ¨¡å—
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=8,
            # dropout=0.5, # 1.8826
            dropout=0.7, # 1.8698
            # dropout=0.8, # 1.8905
            # dropout=0.9, # 1.9377
            batch_first=True
        )
        
        # å¢å¼ºçš„ç‰¹å¾èåˆç½‘ç»œ
        self.feature_fusion = nn.Sequential(
            nn.Linear(embed_dim*3, hidden_dim),
            nn.GELU(),
            # nn.Dropout(0.5), # 1.8826
            nn.Dropout(0.7), # 1.8698
            # nn.Dropout(0.8), # 1.8905
            # nn.Dropout(0.9), # 1.9377
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, embed_dim),
            nn.LayerNorm(embed_dim)
        )
        
        # åŠ¨æ€lambdaç”Ÿæˆå™¨ï¼ˆåŒé€šé“ç»“æ„ï¼‰
        self.lambda_generator = nn.ModuleDict({
            'semantic_branch': nn.Sequential(
                nn.Linear(embed_dim*2, hidden_dim),
                nn.GELU(),
                nn.LayerNorm(hidden_dim)),
            'syntactic_branch': nn.Sequential(
                nn.Linear(embed_dim*2, hidden_dim//2),
                nn.GELU(),
                nn.LayerNorm(hidden_dim//2)),
            'fusion': nn.Sequential(
                nn.Linear(hidden_dim + hidden_dim//2, 1),
                nn.Sigmoid())
        })
        
        # æ®‹å·®è¿æ¥å‚æ•°
        self.alpha = nn.Parameter(torch.tensor(0.1))
        
        # åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        # è·¨æ¨¡æ€æ³¨æ„åŠ›åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.cross_attn.in_proj_weight)
        nn.init.constant_(self.cross_attn.out_proj.bias, 0.0)
        
        # ç‰¹å¾èåˆç½‘ç»œåˆå§‹åŒ–
        nn.init.kaiming_normal_(self.feature_fusion[0].weight, mode='fan_in')
        nn.init.zeros_(self.feature_fusion[-2].weight)

    def forward(self, I, h, h_neg):
        # ç‰¹å¾å½’ä¸€åŒ–
        I_norm = F.normalize(I, p=2, dim=-1)
        h_norm = F.normalize(h, p=2, dim=-1)
        h_neg_norm = F.normalize(h_neg, p=2, dim=-1)
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›äº¤äº’
        attn_output, _ = self.cross_attn(
            query=h_norm.unsqueeze(1),
            key=I_norm.unsqueeze(1),
            value=I_norm.unsqueeze(1)
        )
        h_attn = h_norm + self.alpha * attn_output.squeeze(1)
        
        # å¤šå±‚æ¬¡ç‰¹å¾èåˆ
        fused_feature = self.feature_fusion(
            torch.cat([h_attn, h_neg_norm, h_attn-h_neg_norm], dim=-1)
        )
        
        # åŒé€šé“lambdaç”Ÿæˆ
        semantic_feat = self.lambda_generator['semantic_branch'](torch.cat([fused_feature, h_neg_norm], dim=-1))
        syntactic_feat = self.lambda_generator['syntactic_branch'](torch.cat([h_norm, h_neg_norm], dim=-1))
        lambda_base = self.lambda_generator['fusion'](torch.cat([semantic_feat, syntactic_feat], dim=-1))
        lambda_dynamic = self.lambda_0 * lambda_base
        
        # ç¨³å®šåŒ–å¾—åˆ†è®¡ç®—
        with torch.amp.autocast('cuda', enabled=True):
            scores_H2I = self.logit_scale.exp() * (h_attn @ I_norm.t())
            scores_N2I = self.logit_scale.exp() * (h_neg_norm @ I_norm.t())
            scores = scores_H2I - lambda_dynamic * scores_N2I
            
        return scores.float()  # ç¡®ä¿è¾“å‡ºç²¾åº¦
    
    def calc_losses(self, scores, I, l_pos, img_ids, h=None):
        """
        è’¸é¦è®­ç»ƒ åŠ¨æ€æƒ©ç½šæƒé‡MLP:

        å‚æ•°:
            - cfg: é…ç½®å‚æ•°
            - scores: CLIPGlassesFrame è®¡ç®—å¾—åˆ°çš„ h2I åŒ¹é…å¾—åˆ† [N_caps, N_imgs]
            - I: å›¾åƒç‰¹å¾ [N_imgs, embed_dim]
            - l_pos: è‚¯å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ [N_caps, embed_dim]
            - img_ids: å›¾åƒID [N_imgs]
            - h: CLIPæ–‡æœ¬ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾(EOSç‰¹å¾) [N_caps, embed_dim]
            
        è¿”å›:
            - total_loss: æ€»æŸå¤±
            - loss_dict: å„ä¸ªåˆ†é¡¹æŸå¤±çš„å­—å…¸ | MSEæŸå¤±
        """
        
        # calc similarity
        I = I / I.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
        l_pos = l_pos / l_pos.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
        logit_scale = self.logit_scale.exp()
        scores_gt = logit_scale * l_pos @ I.t() # [batch_size, batch_size]
        
        if h is not None:
            h = h / h.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
            raw_scores = logit_scale * h @ I.t()
            # mse
            print("="*50)
            print(f"G scores: {scores_gt.diag()}")
            print(f"P scores: {scores.diag()}")
            print(f"R scores: {raw_scores.diag()}")
        
        mse_loss = F.mse_loss(scores.diag(), scores_gt.diag(), reduction='none').mean()
        
        # å¤šcaptionæ„ŸçŸ¥æ’åæŸå¤±
        margin = cfg['margin']  # å¯é…ç½®å‚æ•°
        # æ„å»ºå›¾åƒåˆ†ç»„æ©ç 
        _, inverse_indices = torch.unique(img_ids, return_inverse=True)
        group_mask = inverse_indices.unsqueeze(1) == inverse_indices.unsqueeze(0)  # [B,B]
        # è®¡ç®—æ­£æ ·æœ¬å¾—åˆ†ï¼ˆåŒå›¾åƒæ‰€æœ‰captionçš„æœ€å¤§å¾—åˆ†ï¼‰
        pos_scores = torch.where(group_mask, scores, -torch.inf).max(dim=1)[0]  # [B]
        # è®¡ç®—è´Ÿæ ·æœ¬å¾—åˆ†ï¼ˆä¸åŒå›¾åƒæ‰€æœ‰captionçš„æœ€å°å¾—åˆ†ï¼‰
        neg_scores = torch.where(~group_mask, scores, torch.inf).min(dim=1)[0]  # [B]
        # æ’åæŸå¤±è®¡ç®—
        rank_loss = F.relu(neg_scores - pos_scores + margin).mean() * cfg['rank_loss_weight']
        total_loss = mse_loss + rank_loss
        return total_loss, {
            'mse_loss': mse_loss.item(),
            'rank_loss': rank_loss.item(),
        }
    
 
def train(cfg, model:CLIPGlassesFrame, device='cuda'):
    """
    Train the CLIPGlassesFrame model
    
    å‚æ•°:
        - cfg: é…ç½®å‚æ•°
        - model: CLIPGlassesFrameæ¨¡å‹
        - device: è®¾å¤‡ç±»å‹ï¼ˆ'cuda'æˆ–'cpu'ï¼‰
        
    è¿”å›:
        - model: è®­ç»ƒåçš„æ¨¡å‹
    """
            
    if cfg:
        epochs = cfg['epochs']
        batch_size = cfg['batch_size']
        lr = cfg['lr']
        weight_decay = cfg['weight_decay']
        train_size, val_size, test_size = cfg['split']
        num_workers = cfg['num_workers']
        early_stop_patience = cfg['early_stop_patience'] # Early stopping patience
       
    dataset = GlassesDataset(cfg) # Clip_model, Frame_model ç”¨äºé¢„åŠ è½½æ•°æ®è¿‡ç¨‹ä¸­çš„ç‰¹å¾æå–
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    # Move model to device
    model = model.to(device)
    
    # Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['epochs'])
    # scheduler = OneCycleLR(
    #     optimizer, 
    #     max_lr=lr, 
    #     total_steps=epochs*len(train_loader),
    #     pct_start=0.3
    # )

    # Training loop
    for epoch in range(epochs):
        model.train()
        best_loss = float('inf')
        patience_counter = 0 # Early stopping counter
        total_loss = 0
        # losses = {'mse_loss': 0}
        losses = {'mse_loss': 0, 'rank_loss': 0}
        
        for batch in tqdm.tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            h = batch['h'].to(device) # CLIPæ–‡æœ¬ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾(EOSç‰¹å¾) [batch_size, embed_dim]
            level_h_list = batch['level_h_list'].to(device) # [batch_size, num_layers, embed_dim] CLIPæ–‡æœ¬ç¼–ç å™¨æ¯ä¸€å±‚çš„EOSç‰¹å¾
            l_pos = batch['l_pos'].to(device) # è‚¯å®šæ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
            l_neg = batch['neg_obj'].to(device) # è¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
            I = batch['I'].to(device) # å›¾åƒç‰¹å¾ [batch_size, embed_dim]
            img_ids = batch['img_id'].to(device) # å›¾åƒID [batch_size]
            
            # Forward pass
            scores = model(I, h, h_neg=l_neg) # ä½¿ç”¨GT l_neg è®­ç»ƒ MLP
            
            # Compute loss
            loss, loss_dict = model.calc_losses(scores, I, l_pos, img_ids)
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Track metrics
            total_loss += loss.item()
            losses['mse_loss'] += loss_dict['mse_loss']
            losses['rank_loss'] += loss_dict['rank_loss']
        
        # scheduler.step()
        
        # Print epoch summary
        batch_count = len(train_loader)
        # print(f"Ep{epoch+1}/{epochs}  Loss: {total_loss/batch_count:.4f} mse_loss: {losses['mse_loss']/batch_count:.4f}")
        print(f"Ep{epoch+1}/{epochs}  Loss: {total_loss/batch_count:.4f} mse_loss: {losses['mse_loss']/batch_count:.4f} rank_loss: {losses['rank_loss']/batch_count:.4f}")
        
        # Validation
        if epoch % 10 == 0: # æ¯éš”1ä¸ªepochè¿›è¡Œä¸€æ¬¡éªŒè¯
            batch_loss = evaluate(cfg, model, val_loader, vis=True)
        else:
            batch_loss = evaluate(cfg, model, val_loader)
        # æ—©åœ
        if batch_loss < best_loss:
            best_loss = batch_loss
            patience_counter = 0
            torch.save(model.state_dict(), os.path.join(current_dir, 'best_clip_Frame.pth'))
        else:
            patience_counter += 1 # å¢åŠ è€å¿ƒè®¡æ•°å™¨
            print(f"ğŸ’”loss improve from {best_loss:.4f} to {batch_loss:.4f}, cur patience_counter add to {patience_counter}")
            if early_stop_patience > 0 and patience_counter >= early_stop_patience:
                print(f"Early stopping after {epoch+1} epochs")
                break
    return model


def evaluate(cfg, model:CLIPGlassesFrame, data_loader, vis=False, device='cuda'):
    """
    Evaluate the CLIPGlassesFrame model on the validation set
    
    å‚æ•°:
        - cfg: é…ç½®å‚æ•°
        - model: CLIPGlassesFrameæ¨¡å‹
        - data_loader: æ•°æ®åŠ è½½å™¨
        
    è¿”å›:
        - avg_loss: å¹³å‡æŸå¤±
    """
    model.eval()
    total_loss = 0
    losses = {'mse_loss': 0, 'rank_loss': 0}
    
    with torch.no_grad():  # No need to track gradients during evaluation
        for batch in tqdm.tqdm(data_loader, desc="Evaluating"):
            h = batch['h'].to(device)
            level_h_list = batch['level_h_list'].to(device)
            l_pos = batch['l_pos'].to(device)
            l_neg = batch['neg_obj'].to(device)  # Negative object features
            I = batch['I'].to(device)
            img_ids = batch['img_id'].to(device) # å›¾åƒID [batch_size]
            
            # Forward pass
            scores = model(I, h, h_neg=l_neg)
            
            # Compute loss
            if vis:
                loss, loss_dict = model.calc_losses(scores, I, l_pos, img_ids, h)
            else:
                loss, loss_dict = model.calc_losses(scores, I, l_pos, img_ids)
            
            # Track metrics
            total_loss += loss.item()
            losses['mse_loss'] += loss_dict['mse_loss']
            losses['rank_loss'] += loss_dict['rank_loss']
    
    batch_count = len(data_loader)
    avg_loss = total_loss / batch_count
    # print(f"Validation - Loss: {avg_loss:.4f}, MSE Loss: {losses['mse_loss']/batch_count:.4f}")
    print(f"Validation - Loss: {avg_loss:.4f}, mse_loss: {losses['mse_loss']/batch_count:.4f} rank_loss: {losses['rank_loss']/batch_count:.4f}")
    
    return avg_loss

# åŠ è½½æ¨¡å‹å¹¶æµ‹è¯•
def load_model(cfg, model_path):
    """
    Load the trained CLIPGlassesFrame model from a checkpoint
    
    å‚æ•°:
        - cfg: é…ç½®å‚æ•°
        - model_path: æ¨¡å‹è·¯å¾„
        
    è¿”å›:
        - model: åŠ è½½çš„æ¨¡å‹
    """
    model = CLIPGlassesFrame(cfg)
    model.load_state_dict(torch.load(model_path))
    return model


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    cfg = {
        # -----æ¨¡å‹å‚æ•°-----
        'dtype': torch.float32,
        'lambda_0': 0.1, # åŸºç¡€æƒ©ç½šå¼ºåº¦
        'margin': 0.5,
        'rank_loss_weight': 0.5,
        
        # -----è®­ç»ƒå‚æ•°-----
        # 'epochs': 10, # 1.8698
        'epochs': 20,
        # 'batch_size': 32,
        'batch_size': 8,
        # 'lr': 1e-3,
        'lr': 5e-5, # 1.8698
        # 'lr': 1e-5, # 1.8931
        'weight_decay': 1e-4,
        'split': [0.9, 0.1, 0.0], # train val test
        # 'num_workers': 64,
        'num_workers': 4,
        'early_stop_patience': 3,
        
        # -----æ•°æ®å‚æ•°-----
        'pos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_retrieval.csv",
        'negpos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv",
        'split': [0.9, 0.1, 0.0],  # train, val, test split
        'num_workers': 4,
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = CLIPGlassesFrame(cfg)
    
    # è®­ç»ƒæ¨¡å‹
    trained_model = train(cfg, model)

    # model = load_model(cfg, os.path.join(current_dir, 'best_clip_Frame.pth'))
    # model.eval()
    # model = model.to('cuda')
    # data_loader = DataLoader(GlassesDataset(cfg), batch_size=cfg['batch_size'], shuffle=False, num_workers=cfg['num_workers'], drop_last=True)
    # evaluate(cfg, model, data_loader)
    