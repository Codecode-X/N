import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
grandparent_dir = os.path.dirname(parent_dir)
sys.path.insert(0, grandparent_dir)
sys.path.insert(0, parent_dir)
from get_model import Clip_model
from utils import setup_logger, set_random_seed
setup_logger(os.path.join(current_dir, "log.txt")) # å°†è¾“å‡ºé‡å®šå‘åˆ°log.txtæ–‡ä»¶
set_random_seed(3407)  # è®¾ç½®éšæœºç§å­
import torch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import tqdm
from GlassesDataset import GlassesDataset
from torch.utils.data import DataLoader


class CLIPGlassesFrame(nn.Module):   
    """
    CLIPGlassesFrame: 
        è¾“å…¥ï¼š
            - CLIPå›¾åƒç¼–ç å™¨çš„è¾“å‡ºå›¾åƒç‰¹å¾Iã€‚
            - CLIPè¾“å‡ºçš„æ–‡æœ¬ç‰¹å¾hã€‚
            - Lensè¾“å‡ºçš„å¦å®šå†…å®¹æ–‡æœ¬ç‰¹å¾h_negã€‚
        è¾“å‡º:
            - æ–‡æœ¬å’Œå›¾åƒçš„åŒ¹é…åº¦
        åœºæ™¯ï¼š
            - Retrievalä»»åŠ¡
    """
    def __init__(self, cfg, embed_dim=512, hidden_dim=128):
        """
        åˆå§‹åŒ–CLIPGlassesFrameæ¨¡å—
        
        Args:
            - embed_dim: åµŒå…¥ç»´åº¦(CLIPç‰¹å¾ç»´åº¦)
            - hidden_dim: MLPéšè—å±‚ç»´åº¦
            - lambda_0: åŸºç¡€æƒ©ç½šå¼ºåº¦
        """
        super().__init__()
        self.cfg = cfg
        self.lambda_0 = cfg['lambda_0']  # åŸºç¡€æƒ©ç½šå¼ºåº¦
        self.logit_scale = Clip_model.logit_scale.detach() # ç›´æ¥ä½¿ç”¨CLIPæ¨¡å‹çš„è®­ç»ƒå¥½çš„logit_scale
        self.confidence_mlp = nn.Sequential( # ç”¨äºåŠ¨æ€æƒ©ç½šæƒé‡çš„MLP
            nn.Linear(embed_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, I, h, h_neg):
        """
        è®¡ç®—å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„åŒ¹é…å¾—åˆ†
        
        Args:
            I: å›¾åƒç‰¹å¾ [N_imgs, embed_dim]
            h: åŸå†…å®¹æ–‡æœ¬ç‰¹å¾ [N_caps, embed_dim]
            h_neg: å¦å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ [N_caps, embed_dim]
            
        Returns:
            scores: åŒ¹é…å¾—åˆ† [N_caps, N_imgs]
        """
        # è®¡ç®—åŠ¨æ€æƒ©ç½šæƒé‡
        lambda_dynamic = self.lambda_0 * torch.sigmoid(self.confidence_mlp(h_neg)) # [N_caps, 1]
        
        # æ ‡å‡†åŒ–
        I = I / I.norm(dim=-1, keepdim=True) # [N_imgs, embed_dim]
        h = h / h.norm(dim=-1, keepdim=True) # [N_caps, embed_dim]
        h_neg = h_neg / h_neg.norm(dim=-1, keepdim=True) # [N_caps, embed_dim]
        
        # è®¡ç®—æ ‡å‡†åŒ–å·®åˆ†åŒ¹é…å¾—åˆ†
        logit_scale = self.logit_scale.exp()
        scores_H2I = logit_scale * h @ I.t() # [N_caps, N_imgs]
        scores_N2I = logit_scale * h_neg @ I.t() # [N_caps, N_imgs]
        scores = scores_H2I - lambda_dynamic * scores_N2I
        return scores
    
    def calc_losses(self, scores, I, l_pos):
        """
        è’¸é¦è®­ç»ƒ ç”¨äºè®¡ç®—åŠ¨æ€æƒ©ç½šæƒé‡ çš„ MLP:

        å‚æ•°:
            - cfg: é…ç½®å‚æ•°
            - scores: CLIPGlassesFrame è®¡ç®—å¾—åˆ°çš„ h2I åŒ¹é…å¾—åˆ† [N_caps, N_imgs]
            - I: å›¾åƒç‰¹å¾ [N_imgs, embed_dim]
            - l_pos: è‚¯å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ [N_caps, embed_dim]
            
        è¿”å›:
            - total_loss: æ€»æŸå¤±
            - loss_dict: å„ä¸ªåˆ†é¡¹æŸå¤±çš„å­—å…¸ | MSEæŸå¤±
        """
        
        # calc similarity
        I = I / I.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
        l_pos = l_pos / l_pos.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
        logit_scale = self.logit_scale.exp()
        scores_gt = logit_scale * l_pos @ I.t()
        
        # mse
        loss_mse = F.mse_loss(scores, scores_gt, reduction='none')
        
        return loss_mse.mean(), {
            'mse_loss': loss_mse.mean().item(),
        }
 
    
def train(cfg, model:CLIPGlassesFrame, device='cuda'):
    """
    Train the CLIPGlassesLens model
    
    å‚æ•°:
        - cfg: é…ç½®å‚æ•°
        - model: CLIPGlassesLensæ¨¡å‹
        - device: è®¾å¤‡ç±»å‹ï¼ˆ'cuda'æˆ–'cpu'ï¼‰
        
    è¿”å›:
        - model: è®­ç»ƒåçš„æ¨¡å‹
    """
            
    if cfg:
        epochs = cfg['epochs']
        batch_size = cfg['batch_size']
        lr = cfg['lr']
        weight_decay = cfg['weight_decay']
        train_size, val_size, test_size = cfg['split']
        num_workers = cfg['num_workers']
        early_stop_patience = cfg['early_stop_patience'] # Early stopping patience
        
    dataset = GlassesDataset(cfg) # Clip_model, lens_model ç”¨äºé¢„åŠ è½½æ•°æ®è¿‡ç¨‹ä¸­çš„ç‰¹å¾æå–
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    # Move model to device
    model = model.to(device)
    
    # Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['epochs'])

    # Training loop
    for epoch in range(epochs):
        model.train()
        best_loss = float('inf')
        patience_counter = 0 # Early stopping counter
        total_loss = 0
        losses = {'mse_loss': 0}
        
        for batch in tqdm.tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            h = batch['h'].to(device) # CLIPæ–‡æœ¬ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾(EOSç‰¹å¾) [batch_size, embed_dim]
            level_h_list = batch['level_h_list'].to(device) # [batch_size, num_layers, embed_dim] CLIPæ–‡æœ¬ç¼–ç å™¨æ¯ä¸€å±‚çš„EOSç‰¹å¾
            l_pos = batch['l_pos'].to(device) # è‚¯å®šæ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
            l_neg = batch['neg_obj'].to(device) # è¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
            I = batch['I'].to(device) # å›¾åƒç‰¹å¾ [batch_size, embed_dim]
            
            # Forward pass
            scores = model(I, h, h_neg=l_neg) # ä½¿ç”¨GT l_neg è®­ç»ƒ MLP
            
            # Compute loss
            loss, loss_dict = model.calc_losses(scores, I, l_pos)
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Track metrics
            total_loss += loss.item()
            losses['mse_loss'] += loss_dict['mse_loss']
            # losses['pos2negobj_sim_loss'] += loss_dict['pos2negobj_sim_loss']
        
        scheduler.step()
        
        # Print epoch summary
        batch_count = len(train_loader)
        print(f"Ep{epoch+1}/{epochs}  Loss: {total_loss/batch_count:.4f} mse_loss: {losses['mse_loss']/batch_count:.4f}")
        
        # Validation
        batch_loss = evaluate(cfg, model, val_loader)
        
        # æ—©åœ
        if batch_loss < best_loss:
            best_loss = batch_loss
            patience_counter = 0
            torch.save(model.state_dict(), os.path.join(current_dir, 'best_clip_lens.pth'))
        else:
            print(f"ğŸ’”loss improve from {best_loss:.4f} to {batch_loss:.4f}, cur patience_counter add to {patience_counter}")
            patience_counter += 1 # å¢åŠ è€å¿ƒè®¡æ•°å™¨
            if early_stop_patience > 0 and patience_counter >= early_stop_patience:
                print(f"Early stopping after {epoch+1} epochs")
                break
        
    return model


def evaluate(cfg, model, data_loader):
    """
    Evaluate the CLIPGlassesFrame model on the validation set
    
    å‚æ•°:
        - cfg: é…ç½®å‚æ•°
        - model: CLIPGlassesFrameæ¨¡å‹
        - data_loader: æ•°æ®åŠ è½½å™¨
        
    è¿”å›:
        - avg_loss: å¹³å‡æŸå¤±
    """
    model.eval()
    device = next(model.parameters()).device
    total_loss = 0
    losses = {'mse_loss': 0}
    
    with torch.no_grad():  # No need to track gradients during evaluation
        for batch in tqdm.tqdm(data_loader, desc="Evaluating"):
            h = batch['h'].to(device)
            level_h_list = batch['level_h_list'].to(device)
            l_pos = batch['l_pos'].to(device)
            l_neg = batch['neg_obj'].to(device)  # Negative object features
            I = batch['I'].to(device)
            
            # Forward pass
            scores = model(I, h, h_neg=l_neg)
            
            # Compute loss
            loss, loss_dict = model.calc_losses(scores, I, l_pos)
            
            # Track metrics
            total_loss += loss.item()
            losses['mse_loss'] += loss_dict['mse_loss']
    
    batch_count = len(data_loader)
    avg_loss = total_loss / batch_count
    print(f"Validation - Loss: {avg_loss:.4f}, MSE Loss: {losses['mse_loss']/batch_count:.4f}")
    
    return avg_loss


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    cfg = {
        # -----æ¨¡å‹å‚æ•°-----
        'dtype': torch.float32,
        'lambda_0': 0.1, # åŸºç¡€æƒ©ç½šå¼ºåº¦
        
        # -----è®­ç»ƒå‚æ•°-----
        'epochs': 10,
        'batch_size': 32,
        'lr': 1e-4,
        'weight_decay': 1e-4,
        'split': [0.9, 0.1, 0.0], # train val test
        'num_workers': 4,
        'early_stop_patience': 5,
        
        # -----æ•°æ®å‚æ•°-----
        'pos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_retrieval.csv",
        'negpos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv",
        'split': [0.9, 0.1, 0.0],  # train, val, test split
        'num_workers': 4,
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = CLIPGlassesFrame(cfg)
    
    # è®­ç»ƒæ¨¡å‹
    trained_model = train(cfg, model)
