import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
grandparent_dir = os.path.dirname(parent_dir)
sys.path.insert(0, grandparent_dir)
sys.path.insert(0, parent_dir)
from get_model import Clip_model
from utils import setup_logger, set_random_seed
setup_logger(os.path.join(current_dir, "log.txt")) # å°†è¾“å‡ºé‡å®šå‘åˆ°log.txtæ–‡ä»¶
set_random_seed(3407)  # è®¾ç½®éšæœºç§å­
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import tqdm
from GlassesDataset import GlassesDataset
from torch.utils.data import DataLoader

class CLIPGlassesFrame(nn.Module):
    def __init__(self, cfg, embed_dim=512, hidden_dim=2048):
        super().__init__()
        self.cfg = cfg
        self.lambda_0 = cfg['lambda_0']
        self.register_buffer('logit_scale', Clip_model.logit_scale.detach())
        
        # æ·±åº¦è·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼ˆ3å±‚Transformerï¼‰
        self.cross_transformer = nn.TransformerEncoder(
            encoder_layer=nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=8,
                dim_feedforward=hidden_dim,
                dropout=0.3,
                activation='gelu',
                batch_first=True,
                layer_norm_eps=1e-6 # å¢åŠ æ•°å€¼ç¨³å®šæ€§
            ),
            num_layers=3
        )
        
        # å¤šæ¨¡æ€ç‰¹å¾èåˆç½‘ç»œï¼ˆå¸¦é—¨æ§æ®‹å·®è¿æ¥ï¼‰
        self.feature_fusion = nn.Sequential(
            *[ResidualBlock(embed_dim*3, hidden_dim) for _ in range(2)],
            nn.Linear(embed_dim*3, embed_dim),  # æ–°å¢ç»´åº¦å‹ç¼©å±‚
            nn.LayerNorm(embed_dim)
        )
        
        # åŠ¨æ€lambdaç”Ÿæˆå™¨ï¼ˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼‰
        self.lambda_generator = nn.ModuleDict({
            'cross_attn': nn.MultiheadAttention(
                embed_dim=embed_dim,
                num_heads=4,
                dropout=0.2,
                batch_first=True
            ),
            'gate_controller': nn.Sequential(
                nn.Linear(2*embed_dim, 1),
                nn.Sigmoid()
            )
        })
        
        # è‡ªé€‚åº”æ®‹å·®ç³»æ•°
        self.alpha = nn.Parameter(torch.ones(2)*0.1)  # å¤šé˜¶æ®µæ®‹å·®
        
        # åˆå§‹åŒ–é€‚é…
        self._init_weights()

    def _init_weights(self):
        # Transformeråˆå§‹åŒ–
        for p in self.cross_transformer.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        
        # ç‰¹å¾èåˆç½‘ç»œåˆå§‹åŒ–
        for module in self.feature_fusion:
            if isinstance(module, ResidualBlock):
                nn.init.kaiming_normal_(module.fc1.weight, mode='fan_in')
                nn.init.zeros_(module.fc2.weight)
                nn.init.xavier_normal_(module.gate.weight)
            elif isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_in')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, I, h, h_neg, neg_mask=None):
        """
        å‚æ•°ï¼š
            - I: å›¾åƒç‰¹å¾ [batch_size, embed_dim]
            - h: CLIPæ–‡æœ¬ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾(EOSç‰¹å¾) [batch_size, embed_dim]
            - h_neg: è¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
            - neg_mask: æ˜¯å¦æœ‰å¦å®šå¯¹è±¡mask [batch_size] | 1:æœ‰å¦å®šå¯¹è±¡ | 0: æ— å¦å®šå¯¹è±¡
        
        è¿”å›ï¼š
            - scores: CLIPGlassesFrame è®¡ç®—å¾—åˆ°çš„ h2I åŒ¹é…å¾—åˆ† [N_caps, N_imgs]
        """
        # ç‰¹å¾å½’ä¸€åŒ–
        I_norm = F.normalize(I, p=2, dim=-1)
        h_norm = F.normalize(h, p=2, dim=-1)
        h_neg_norm = F.normalize(h_neg, p=2, dim=-1) + 1e-8
        
        # æ·±åº¦è·¨æ¨¡æ€äº¤äº’
        cross_features = self.cross_transformer(
            torch.cat([h_norm.unsqueeze(1), I_norm.unsqueeze(1)], dim=1)
        )
        h_attn = self.alpha[0]*h_norm + cross_features[:,0]
        
        # å¤šæ¨¡æ€ç‰¹å¾èåˆ
        fused_feature = self.feature_fusion(
            torch.cat([
                h_attn, 
                h_neg_norm, 
                h_attn * h_neg_norm  # æ–°å¢äº¤äº’ç‰¹å¾
            ], dim=-1)
        )
        
        # åŠ¨æ€lambdaç”Ÿæˆï¼ˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼‰
        attn_output, _ = self.lambda_generator['cross_attn'](
            query=h_attn.unsqueeze(1),
            key=h_neg_norm.unsqueeze(1),
            value=h_neg_norm.unsqueeze(1)
        )
        gate_input = torch.cat([h_attn, attn_output.squeeze(1)], dim=-1)
        lambda_base = self.lambda_generator['gate_controller'](gate_input)
        lambda_dynamic = torch.sigmoid(self.lambda_0 * lambda_base) # åŠ¨æ€lambdaç”Ÿæˆå™¨ï¼Œé™åˆ¶åœ¨[0,1]ä¹‹é—´
        
        # ä¿æŒCLIPåŸºç¡€èƒ½åŠ›çš„è‡ªé€‚åº”åŒ¹é…
        with torch.amp.autocast('cuda', enabled=True):
            logit_scale = self.logit_scale.exp()
            
            # åŸCLIPé¢„æµ‹çš„hå’ŒIçš„åŒ¹é…åˆ†æ•° 
            scores_base = logit_scale * (h_norm @ I_norm.t())
            
            # å¢å¼ºåŒ¹é…è·¯å¾„
            enhanced_feature = self.alpha[0]*h_norm + self.alpha[1]*fused_feature
            scores_enhanced = logit_scale * (enhanced_feature @ I_norm.t())
            
            # å¦å®šæ„ŸçŸ¥è°ƒæ•´
            scores_N2I = logit_scale * (h_neg_norm @ I_norm.t())
            adjusted_scores = scores_enhanced - lambda_dynamic * scores_N2I
            
            # æ¡ä»¶æ··åˆ
            if neg_mask is not None:
                neg_mask = neg_mask.to(scores_base.dtype)
                scores = torch.where(
                    neg_mask.bool().view(-1,1),  # å°†maskè½¬æ¢ä¸º[B,1]ç”¨äºè¡Œå¹¿æ’­
                    adjusted_scores + scores_base.detach(),  # Trueæ—¶ä½¿ç”¨ä¿®æ­£åˆ†æ•°
                    scores_base  # Falseæ—¶ä½¿ç”¨åŸå§‹åˆ†æ•°
                )
            else:
                scores = adjusted_scores + scores_base.detach() # æ— maskæ—¶ä¿æŒåŸæœ‰é€»è¾‘ | scores_base.detach() é˜²æ­¢æ¢¯åº¦å›ä¼ åˆ°åŸå§‹CLIPæ¨¡å‹
        
        return scores.float()
    
    @staticmethod
    def load_model(cfg):
        """
        Load the trained CLIPGlassesFrame model from a checkpoint
        
        å‚æ•°:
            - cfg: é…ç½®å‚æ•°
            - model_path: æ¨¡å‹è·¯å¾„
            
        è¿”å›:
            - model: åŠ è½½çš„æ¨¡å‹
        """
        model = CLIPGlassesFrame(cfg)
        if 'model_path' in cfg.keys() and cfg['model_path'] is not None:
            print(f"æ­£åœ¨åŠ è½½ CLIPGlassesFrame æ¨¡å‹æƒé‡: {cfg['model_path']}")
            model.load_state_dict(torch.load(cfg['model_path'], weights_only=False))
        model = model.to(cfg['device'])
        model.eval()
        return model

class ResidualBlock(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, input_dim)
        self.gate = nn.Linear(input_dim, 1)
        self.layer_norm = nn.LayerNorm(input_dim)
        
    def forward(self, x):
        residual = x
        x = F.gelu(self.fc1(x))
        x = F.dropout(x, p=0.3, training=self.training)
        x = torch.clamp(x, min=-5.0, max=5.0)  # æ¢¯åº¦æˆªæ–­
        x = self.fc2(x)
        
        # é—¨æ§æ®‹å·®è¿æ¥
        gate = torch.sigmoid(self.gate(residual))
        return self.layer_norm(gate * x + (1 - gate) * residual)
    
    
    def calc_losses(self, scores, I, l_pos, img_ids, h=None):
        """
        è’¸é¦è®­ç»ƒ åŠ¨æ€æƒ©ç½šæƒé‡MLP:

        å‚æ•°:
            - cfg: é…ç½®å‚æ•°
            - scores: CLIPGlassesFrame è®¡ç®—å¾—åˆ°çš„ h2I åŒ¹é…å¾—åˆ† [N_caps, N_imgs]
            - I: å›¾åƒç‰¹å¾ [N_imgs, embed_dim]
            - l_pos: è‚¯å®šå†…å®¹æ–‡æœ¬ç‰¹å¾ [N_caps, embed_dim]
            - img_ids: å›¾åƒID [N_imgs]
            - h: CLIPæ–‡æœ¬ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾(EOSç‰¹å¾) [N_caps, embed_dim]
            
        è¿”å›:
            - total_loss: æ€»æŸå¤±
            - loss_dict: å„ä¸ªåˆ†é¡¹æŸå¤±çš„å­—å…¸ | MSEæŸå¤±
        """
        
        # calc similarity
        I = I / I.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
        l_pos = l_pos / l_pos.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
        logit_scale = self.logit_scale.exp()
        scores_gt = logit_scale * l_pos @ I.t() # [batch_size, batch_size]
        
        if h is not None:
            h = h / h.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
            raw_scores = logit_scale * h @ I.t()
            # mse
            print("="*50)
            print(f"G scores: {scores_gt.diag()}")
            print(f"P scores: {scores.diag()}")
            print(f"R scores: {raw_scores.diag()}")
        
        mse_loss = F.mse_loss(scores.diag(), scores_gt.diag(), reduction='none').mean()
        
        # å¤šcaptionæ„ŸçŸ¥æ’åæŸå¤±
        margin = cfg['margin']  # å¯é…ç½®å‚æ•°
        # æ„å»ºå›¾åƒåˆ†ç»„æ©ç 
        _, inverse_indices = torch.unique(img_ids, return_inverse=True)
        group_mask = inverse_indices.unsqueeze(1) == inverse_indices.unsqueeze(0)  # [B,B]
        # è®¡ç®—æ­£æ ·æœ¬å¾—åˆ†ï¼ˆåŒå›¾åƒæ‰€æœ‰captionçš„æœ€å¤§å¾—åˆ†ï¼‰
        pos_scores = torch.where(group_mask, scores, -torch.inf).max(dim=1)[0]  # [B]
        # è®¡ç®—è´Ÿæ ·æœ¬å¾—åˆ†ï¼ˆä¸åŒå›¾åƒæ‰€æœ‰captionçš„æœ€å°å¾—åˆ†ï¼‰
        neg_scores = torch.where(~group_mask, scores, torch.inf).min(dim=1)[0]  # [B]
        # æ’åæŸå¤±è®¡ç®—
        rank_loss = F.relu(neg_scores - pos_scores + margin).mean() * cfg['rank_loss_weight']
        total_loss = mse_loss + rank_loss
        return total_loss, {
            'mse_loss': mse_loss.item(),
            'rank_loss': rank_loss.item(),
        }
        
    @staticmethod
    def load_model(cfg):
        """
        Load the trained CLIPGlassesFrame model from a checkpoint
        
        å‚æ•°:
            - cfg: é…ç½®å‚æ•°
            - model_path: æ¨¡å‹è·¯å¾„
            
        è¿”å›:
            - model: åŠ è½½çš„æ¨¡å‹
        """
        model = CLIPGlassesFrame(cfg)
        if 'model_path' in cfg.keys() and cfg['model_path'] is not None:
            print(f"æ­£åœ¨åŠ è½½ CLIPGlassesFrame æ¨¡å‹æƒé‡: {cfg['model_path']}")
            model.load_state_dict(torch.load(cfg['model_path'], weights_only=False))
        model = model.to(cfg['device'])
        model.eval()
        return model
    
 
def train(cfg, model:CLIPGlassesFrame, Lens_model=None, device='cuda'):
    """
    Train the CLIPGlassesFrame model
    
    å‚æ•°:
        - cfg: é…ç½®å‚æ•°
        - model: CLIPGlassesFrameæ¨¡å‹
        - Lens_model: Lens_model=None è¡¨ç¤ºä½¿ç”¨GT neg_objè¿›è¡Œè®­ç»ƒï¼Œå¦åˆ™ä½¿ç”¨å†»ç»“çš„Lensæ¨¡å‹é¢„æµ‹çš„neg_objè¿›è¡Œè®­ç»ƒ
        - device: è®¾å¤‡ç±»å‹ï¼ˆ'cuda'æˆ–'cpu'ï¼‰
        
    è¿”å›:
        - model: è®­ç»ƒåçš„æ¨¡å‹
    """
            
    if cfg:
        epochs = cfg['epochs']
        batch_size = cfg['batch_size']
        lr = cfg['lr']
        weight_decay = cfg['weight_decay']
        train_size, val_size, test_size = cfg['split']
        num_workers = cfg['num_workers']
        early_stop_patience = cfg['early_stop_patience'] # Early stopping patience
       
    dataset = GlassesDataset(cfg) # Clip_model, Frame_model ç”¨äºé¢„åŠ è½½æ•°æ®è¿‡ç¨‹ä¸­çš„ç‰¹å¾æå–
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    # Move model to device
    model = model.to(device)
    
    # Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['epochs'])
    # scheduler = OneCycleLR(
    #     optimizer, 
    #     max_lr=lr, 
    #     total_steps=epochs*len(train_loader),
    #     pct_start=0.3
    # )

    # Training loop
    patience_counter = 0 # Early stopping counter
    for epoch in range(epochs):
        model.train()
        best_loss = float('inf')
        total_loss = 0
        # losses = {'mse_loss': 0}
        losses = {'mse_loss': 0, 'rank_loss': 0}
        
        for batch in tqdm.tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            h = batch['h'].to(device) # CLIPæ–‡æœ¬ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾(EOSç‰¹å¾) [batch_size, embed_dim]
            level_h_list = batch['level_h_list'].to(device) # [batch_size, num_layers, embed_dim] CLIPæ–‡æœ¬ç¼–ç å™¨æ¯ä¸€å±‚çš„EOSç‰¹å¾
            l_pos = batch['l_pos'].to(device) # è‚¯å®šæ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
            l_neg = batch['neg_obj'].to(device) # è¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
            I = batch['I'].to(device) # å›¾åƒç‰¹å¾ [batch_size, embed_dim]
            img_ids = batch['img_id'].to(device) # å›¾åƒID [batch_size]
            
            # Forward pass
            if Lens_model is None: # ä½¿ç”¨GT neg_obj è®­ç»ƒ MLP
                scores = model(I, h, h_neg=l_neg) # ä½¿ç”¨GT l_neg è®­ç»ƒ MLP
            else: # ä½¿ç”¨å†»ç»“çš„Lensæ¨¡å‹é¢„æµ‹çš„neg_objè¿›è¡Œè®­ç»ƒ
                with torch.no_grad():
                    h_neg = Lens_model(h, level_h_list)
                scores = model(I, h, h_neg=h_neg) # ä½¿ç”¨Lensæ¨¡å‹é¢„æµ‹çš„neg_objè¿›è¡Œè®­ç»ƒ

            # Compute loss
            loss, loss_dict = model.calc_losses(scores, I, l_pos, img_ids)
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Track metrics
            total_loss += loss.item()
            losses['mse_loss'] += loss_dict['mse_loss']
            losses['rank_loss'] += loss_dict['rank_loss']
        
        # scheduler.step()
        
        # Print epoch summary
        batch_count = len(train_loader)
        # print(f"Ep{epoch+1}/{epochs}  Loss: {total_loss/batch_count:.4f} mse_loss: {losses['mse_loss']/batch_count:.4f}")
        print(f"Ep{epoch+1}/{epochs}  Loss: {total_loss/batch_count:.4f} mse_loss: {losses['mse_loss']/batch_count:.4f} rank_loss: {losses['rank_loss']/batch_count:.4f}")
        
        # Validation
        if epoch % 10 == 0: # æ¯éš”1ä¸ªepochè¿›è¡Œä¸€æ¬¡éªŒè¯
            batch_loss = evaluate(cfg, model, val_loader, vis=True)
        else:
            batch_loss = evaluate(cfg, model, val_loader)
        # æ—©åœ
        if batch_loss < best_loss:
            best_loss = batch_loss
            patience_counter = 0
            torch.save(model.state_dict(), os.path.join(current_dir, cfg['save_path']))
        else:
            patience_counter += 1 # å¢åŠ è€å¿ƒè®¡æ•°å™¨
            print(f"ğŸ’”loss improve from {best_loss:.4f} to {batch_loss:.4f}, cur patience_counter add to {patience_counter}")
            if early_stop_patience > 0 and patience_counter >= early_stop_patience:
                print(f"Early stopping after {epoch+1} epochs")
                break
    return model


def evaluate(cfg, model:CLIPGlassesFrame, data_loader, vis=False, device='cuda'):
    """
    Evaluate the CLIPGlassesFrame model on the validation set
    
    å‚æ•°:
        - cfg: é…ç½®å‚æ•°
        - model: CLIPGlassesFrameæ¨¡å‹
        - data_loader: æ•°æ®åŠ è½½å™¨
        
    è¿”å›:
        - avg_loss: å¹³å‡æŸå¤±
    """
    model.eval()
    total_loss = 0
    losses = {'mse_loss': 0, 'rank_loss': 0}
    
    with torch.no_grad():  # No need to track gradients during evaluation
        for batch in tqdm.tqdm(data_loader, desc="Evaluating"):
            h = batch['h'].to(device)
            level_h_list = batch['level_h_list'].to(device)
            l_pos = batch['l_pos'].to(device)
            l_neg = batch['neg_obj'].to(device)  # Negative object features
            I = batch['I'].to(device)
            img_ids = batch['img_id'].to(device) # å›¾åƒID [batch_size]
            
            # Forward pass
            scores = model(I, h, h_neg=l_neg)
            
            # Compute loss
            if vis:
                loss, loss_dict = model.calc_losses(scores, I, l_pos, img_ids, h)
            else:
                loss, loss_dict = model.calc_losses(scores, I, l_pos, img_ids)
            
            # Track metrics
            total_loss += loss.item()
            losses['mse_loss'] += loss_dict['mse_loss']
            losses['rank_loss'] += loss_dict['rank_loss']
    
    batch_count = len(data_loader)
    avg_loss = total_loss / batch_count
    # print(f"Validation - Loss: {avg_loss:.4f}, MSE Loss: {losses['mse_loss']/batch_count:.4f}")
    print(f"Validation - Loss: {avg_loss:.4f}, mse_loss: {losses['mse_loss']/batch_count:.4f} rank_loss: {losses['rank_loss']/batch_count:.4f}")
    
    return avg_loss


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    cfg = {
        # -----æ¨¡å‹å‚æ•°-----
        'dtype': torch.float32,
        'device': 'cuda',
        'lambda_0': 0.1, # åŸºç¡€æƒ©ç½šå¼ºåº¦
        
        'model_path': os.path.join(current_dir, 'weights/best_clip_Frame_mse_v1869.pth'), # é¢„è®­ç»ƒæ¨¡å‹æƒé‡çš„è·¯å¾„
        'save_path': os.path.join(current_dir, 'best_clip_Frame.pth'), # è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹æƒé‡ä¿å­˜è·¯å¾„
        
        'rank_loss_weight': 0.5, # æ’åæŸå¤±æƒé‡
        'margin': 0.5, # æ’åæŸå¤±çš„margin
        
        'Lens': {
            'device': 'cuda',
            'dtype': torch.float32,
            'num_heads': 4,
            'dropout': 0.1,
            'model_path': '/root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_clip_lens_9832_0027.pth' # Lensçš„é¢„è®­ç»ƒæƒé‡
        },
        
        # -----è®­ç»ƒå‚æ•°-----
        # 'epochs': 10, # 1.8698
        'epochs': 20,
        # 'batch_size': 32,
        'batch_size': 8,
        # 'lr': 1e-3,
        'lr': 5e-5, # 1.8698
        # 'lr': 1e-5, # 1.8931
        'weight_decay': 1e-4,
        'split': [0.9, 0.1, 0.0], # train val test
        # 'num_workers': 64,
        'num_workers': 4,
        'early_stop_patience': 3,
        
        # -----æ•°æ®å‚æ•°-----
        'pos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_retrieval.csv",
        'negpos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv",
        'split': [0.9, 0.1, 0.0],  # train, val, test split
        'num_workers': 4,
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = CLIPGlassesFrame(cfg)
    
    # åŠ è½½å½“å‰Frameæ¨¡å‹é¢„è®­ç»ƒæƒé‡
    if cfg['model_path'] is not None:
        print(f"æ­£åœ¨åŠ è½½ CLIPGlassesFrame æ¨¡å‹æƒé‡: {cfg['model_path']}")
        model.load_state_dict(torch.load(cfg['model_path'], weights_only=True))
    
    # åŠ è½½å†»ç»“çš„é¢„è®­ç»ƒçš„Lensæ¨¡å‹
    from Lens import CLIPGlassesLens
    lens_model = CLIPGlassesLens.load_model(cfg['Lens'])
    for param in lens_model.parameters():
        param.requires_grad = False
    lens_model.eval()
    
    # è®­ç»ƒæ¨¡å‹
    trained_model = train(cfg, model) # ç›´æ¥ä½¿ç”¨ GT neg_obj è¿›è¡Œè®­ç»ƒ
    # trained_model = train(cfg, model, lens_model) # ä½¿ç”¨å†»ç»“çš„Lensæ¨¡å‹é¢„æµ‹çš„ neg_obj è¿›è¡Œè®­ç»ƒ

    # model = CLIPGlassesFrame.load_model(cfg)
    # model.eval()
    # model = model.to('cuda')
    # data_loader = DataLoader(GlassesDataset(cfg), batch_size=cfg['batch_size'], shuffle=False, num_workers=cfg['num_workers'], drop_last=True)
    # evaluate(cfg, model, data_loader)
    