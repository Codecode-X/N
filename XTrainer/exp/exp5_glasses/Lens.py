import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
from get_model import Clip_model, extract_sentence_features
from utils import setup_logger, set_random_seed
setup_logger(os.path.join(current_dir, "log.txt")) # å°†è¾“å‡ºé‡å®šå‘åˆ°log.txtæ–‡ä»¶
set_random_seed(3407)  # è®¾ç½®éšæœºç§å­
import torch
import tqdm
import numpy as np
import pandas as pd
import random
import torch.nn as nn
import torch.nn.functional as F
from GlassesDataset import GlassesDataset
from torch.utils.data import DataLoader
import torch.optim as optim

class CLIPGlassesLens(nn.Module):
    """
    ç”¨äºä»CLIPæ–‡æœ¬ç¼–ç å™¨è¾“å‡ºçš„æœ€ç»ˆæ–‡æœ¬ç‰¹å¾(h)æˆ–ä¸­é—´å±‚ç‰¹å¾(level_h_list)ä¸­æå–å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾(h_neg)ã€‚
    
    æ¨¡å‹è®¾è®¡:
    å‰ä¸‰å±‚çš„ç‰¹å¾ç”¨äºè¯­æ³•æµï¼Œåˆ†åˆ«ä¹˜ä»¥ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°w1ã€w2ã€w3ï¼Œç”ŸæˆQ1ã€Q2ã€Q3ã€‚
    æœ€åä¸€å±‚çš„ç‰¹å¾ç”¨äºè¯­ä¹‰æµï¼Œä¹˜ä»¥ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°w4ï¼Œç”ŸæˆKï¼Œ ä¹˜ä»¥ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°w5ï¼Œw6,w7ï¼Œç”ŸæˆV1ã€V2ã€V3ã€‚
    Q1ã€Q2ã€Q3 å’Œ K è¿›è¡Œç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡çŸ©é˜µA1ã€A2ã€A3ã€‚
    A1ã€A2ã€A3 å’Œ V1ã€V2ã€V3 è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„å¦å®šå¯¹è±¡æ–‡æœ¬ç‰¹å¾c_negã€‚
    c_neg ç»è¿‡ä¸€ä¸ªffnå±‚ï¼Œå¾—åˆ°æœ€ç»ˆçš„å¦å®šå¯¹è±¡æ–‡æœ¬ç‰¹å¾ h_negã€‚
    
    æ¨¡å‹è®­ç»ƒ:
    - ç›‘ç£ä¿¡å·:
        - neg_objï¼š å¦å®šå¯¹è±¡æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
        - neg_objï¼š è‚¯å®šå¯¹è±¡æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
    - æŸå¤±å‡½æ•°:
        - 1-neg_sim.mean() ç›®æ ‡æ˜¯æœ€å¤§åŒ–é¢„æµ‹çš„h_negå’ŒçœŸå®çš„neg_objä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
    """
    def __init__(self, cfg):
        super().__init__()
        self.embed_dim = 512
        
        # æ®‹å·®è¿æ¥é—¨æ§æœºåˆ¶
        self.res_gate = nn.Sequential(
            nn.Linear(self.embed_dim, 1),
            nn.Sigmoid()  # ç›´æ¥å†…ç½®Sigmoid
        )
        
        # è¯­æ³•æµå‚æ•° (å‰ä¸‰å±‚)
        self.syntax_proj = nn.ModuleList([
            nn.Sequential(
                nn.Linear(self.embed_dim, self.embed_dim, bias=False),
                nn.GELU(),
                nn.LayerNorm(self.embed_dim)
            ) for _ in range(3)
        ])
        
        # è¯­ä¹‰æµå‚æ•° (æœ€åä¸€å±‚)
        self.semantic_k_proj = nn.Sequential(
            nn.Linear(self.embed_dim, self.embed_dim, bias=False),
            nn.GELU(),
            nn.LayerNorm(self.embed_dim)
        )
        self.semantic_v_proj = nn.ModuleList([
            nn.Sequential(
                nn.Linear(self.embed_dim, self.embed_dim, bias=False),
                nn.GELU(),
                nn.LayerNorm(self.embed_dim)
            ) for _ in range(3)
        ])
        
        # æ³¨æ„åŠ›èåˆæ¨¡å—
        attention_scale = torch.tensor([1.0, 0.8, 0.6])
        self.attention_log_scale = nn.Parameter(torch.log(attention_scale))
        
        # FFNç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(self.embed_dim, 2*self.embed_dim),
            nn.LayerNorm(2*self.embed_dim),  # æ–°å¢å±‚å½’ä¸€åŒ–
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(2*self.embed_dim, self.embed_dim),
            nn.LayerNorm(self.embed_dim)
        )
        
        # åˆå§‹åŒ–å‚æ•°
        self._init_weights()
        
    def _init_weights(self):
        # æ®‹å·®é—¨æ§å±‚åˆå§‹åŒ–
        nn.init.constant_(self.res_gate[0].bias, -2.0)  # åˆå§‹åå‘åŸå§‹ç‰¹å¾
        nn.init.normal_(self.res_gate[0].weight, mean=0.0, std=0.02)
        
        # è¯­æ³•æµæŠ•å½±å±‚åˆå§‹åŒ–ï¼ˆGELUï¼‰
        for proj in self.syntax_proj:
            nn.init.kaiming_normal_(
                proj[0].weight, 
                mode='fan_in',
                nonlinearity='relu'
            )
        
        # è¯­ä¹‰æµæŠ•å½±å±‚åˆå§‹åŒ–ï¼ˆGELUï¼‰
        for proj in self.semantic_v_proj:
            nn.init.kaiming_normal_(
                proj[0].weight, 
                mode='fan_in',
                nonlinearity='relu'
            )
         
            
    @staticmethod        
    def load_model(cfg):
        model = CLIPGlassesLens(cfg)
        if 'model_path' in cfg.keys() and cfg['model_path'] is not None:
            print(f"æ­£åœ¨åŠ è½½ CLIPGlassesLens æ¨¡å‹æƒé‡: {cfg['model_path']}")
            model.load_state_dict(torch.load(cfg['model_path'], weights_only=True))
        model = model.to(cfg['device'])
        model.eval()
        return model
        
    def forward(self, h, level_h_list):
        """
        å‚æ•°:
            - h: æœ€åä¸€å±‚ç‰¹å¾ [B, D]
            - level_h_list: å„å±‚ç‰¹å¾åˆ—è¡¨ [B, L, D]
        è¿”å›:
            - h_neg: å¦å®šå¯¹è±¡æ–‡æœ¬ç‰¹å¾ [B, D]
        """
        # è¯­æ³•æµå¤„ç† (å‰ä¸‰å±‚)
        syntax_feats = []
        for i in range(3):
            layer_feat = level_h_list[:, i, :]  # ç¬¬iå±‚ç‰¹å¾
            proj_feat = self.syntax_proj[i](layer_feat)  # [B, D]
            syntax_feats.append(proj_feat)
        
        # è¯­ä¹‰æµå¤„ç† (æœ€åä¸€å±‚)
        K = self.semantic_k_proj(h)  # [B, D] | K (key)
        Vs = [proj(h) for proj in self.semantic_v_proj]  # 3x[B, D] | V1,V2,V3 (value)
        
        # åˆ†å±‚æ³¨æ„åŠ›è®¡ç®—
        c_neg = 0
        scale_factor = torch.sqrt(torch.tensor(self.embed_dim, dtype=torch.float32)) # æ³¨æ„åŠ›ç¼©æ”¾å› å­
        for i in range(3):
            Q = syntax_feats[i] # [B, D] | Q1,Q2,Q3 (query)
            A = torch.bmm(Q.unsqueeze(1), K.unsqueeze(-1)).squeeze(-1) / scale_factor  # [B, 1]
            # A = F.softmax(A * self.attention_scale[i], dim=0)  # å½’ä¸€åŒ–æ³¨æ„åŠ›æƒé‡
            A = F.softmax(A * self.attention_log_scale[i], dim=0)  # å½’ä¸€åŒ–æ³¨æ„åŠ›æƒé‡
            c_neg += A * Vs[i]
        
        # æ®‹å·®è¿æ¥å¹¶æ·»åŠ é—¨æ§æœºåˆ¶
        gate = torch.sigmoid(self.res_gate(c_neg)) 
        c_neg = gate * c_neg + (1 - gate) * h  # æ·»åŠ å¯å­¦ä¹ çš„æ®‹å·®é—¨æ§
        
        # FFNå¤„ç†
        h_neg = self.ffn(c_neg)
        
        return h_neg

    def calc_losses(self, h_neg, neg_obj, I, h=None):
        """
        Args:
            h_neg: é¢„æµ‹çš„å¦å®šç‰¹å¾ [B,D]
            neg_obj: çœŸå®å¦å®šå¯¹è±¡ç‰¹å¾ [B,D]
            I: å›¾åƒç‰¹å¾ [B,D]
            pos_obj: çœŸå®è‚¯å®šå¯¹è±¡ç‰¹å¾ [B,D]
            h_original: åŸå§‹ç‰¹å¾ [B,D]
        """
        h_neg_n = F.normalize(h_neg, p=2, dim=-1)
        neg_obj_n = F.normalize(neg_obj, p=2, dim=-1)
        I_n = F.normalize(I, p=2, dim=-1)
        h_n = F.normalize(h, p=2, dim=-1)
        
        neg_sim = (h_neg_n * neg_obj_n).sum(dim=-1)  # [B] è¶Šé«˜è¶Šå¥½
        neg_sim_loss = 1 - neg_sim.mean()  # è¶Šä½è¶Šå¥½
        
        obj2i_sim = (neg_obj_n * I_n).sum(dim=-1)  # [B] è¶Šé«˜è¶Šå¥½
        
        n2i_sim = (h_neg_n * I_n).sum(dim=-1)  # [B] åº”è¯¥å’Œobj2i_simè¶Šæ¥è¿‘è¶Šå¥½ | ä¿è¯ä¸å›¾åƒç‰¹å¾çš„å¯¹é½
        n2i_sim_loss =  abs(obj2i_sim - n2i_sim).mean()  # è¶Šä½è¶Šå¥½
        
        total_loss = neg_sim_loss + n2i_sim_loss  # è¶Šä½è¶Šå¥½
        
        return total_loss, {
            'neg_sim_loss': neg_sim_loss.item(),
            'n2i_sim_loss': n2i_sim_loss.item(),
        }

def train(cfg, model:CLIPGlassesLens, device='cuda'):
    """Train the CLIPGlassesLens model"""
            
    if cfg:
        epochs = cfg['epochs']
        batch_size = cfg['batch_size']
        lr = cfg['lr']
        weight_decay = cfg['weight_decay']
        train_size, val_size, test_size = cfg['split']
        num_workers = cfg['num_workers']
        early_stop_patience = cfg['early_stop_patience'] # Early stopping patience
        
    dataset = GlassesDataset(cfg) # Clip_model, lens_model ç”¨äºé¢„åŠ è½½æ•°æ®è¿‡ç¨‹ä¸­çš„ç‰¹å¾æå–
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    # Move model to device
    model = model.to(device)
    
    # Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['epochs'])

    # Training loop
    for epoch in range(epochs):
        model.train()
        best_sim_loss = float('inf')
        patience_counter = 0 # Early stopping counter
        total_loss = 0
        losses = {'neg_sim_loss': 0, 'n2i_sim_loss': 0}
        
        for batch in tqdm.tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            I = batch['I'].to(device) # å›¾åƒç‰¹å¾ [batch_size, embed_dim]
            h = batch['h'].to(device) # CLIPæ–‡æœ¬ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾(EOSç‰¹å¾) [batch_size, embed_dim]
            level_h_list = batch['level_h_list'].to(device) # [batch_size, num_layers, embed_dim] CLIPæ–‡æœ¬ç¼–ç å™¨æ¯ä¸€å±‚çš„EOSç‰¹å¾
            neg_obj = batch['neg_obj'].to(device) # [batch_size, num_objs, embed_dim]
            
            # Forward pass
            h_neg = model(h, level_h_list)
            
            # Compute loss
            loss, loss_dict = model.calc_losses(h_neg, neg_obj, I, h)
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Track metrics
            total_loss += loss.item()
            losses['neg_sim_loss'] += loss_dict['neg_sim_loss']
            losses['n2i_sim_loss'] += loss_dict['n2i_sim_loss']
        
        scheduler.step()
        
        # Print epoch summary
        batch_count = len(train_loader)
        print(f"Ep{epoch+1}/{epochs}  Loss: {total_loss/batch_count:.4f}  \
                neg_sim_loss: {losses['neg_sim_loss']/batch_count:.4f} \
                n2i_sim_loss: {losses['n2i_sim_loss']/batch_count:.4f}")
        
        # Validation
        batch_sim_loss = evaluate(cfg, model, val_loader)
        
        # æ—©åœ 
        if batch_sim_loss < best_sim_loss:
            best_sim_loss = batch_sim_loss
            patience_counter = 0
            torch.save(model.state_dict(), os.path.join(current_dir, cfg['save_path']))
        else:
            print(f"ğŸ’”loss improve from {best_sim_loss:.4f} to {batch_sim_loss:.4f}, cur patience_counter add to {patience_counter}")
            patience_counter += 1 # å¢åŠ è€å¿ƒè®¡æ•°å™¨
            if early_stop_patience > 0 and patience_counter >= early_stop_patience:
                print(f"Early stopping after {epoch+1} epochs")
                break
        
    return model


def evaluate(cfg, model, data_loader, device='cuda'):
    """
    Evaluate the CLIPGlassesLens model
    
    å‚æ•°ï¼š
        - cfg: é…ç½®å‚æ•°
        - model: CLIPGlassesLensæ¨¡å‹
        - data_loader: æ•°æ®åŠ è½½å™¨
        - device: è®¾å¤‡ï¼ˆ'cuda'æˆ–'cpu'ï¼‰
        
    è¿”å›ï¼š
        - None
    """
    model.eval()
    model = model.to(device)
    p_sim_total = 0
    h_sim_total = 0
    neg_sim_total = 0
    
    with torch.no_grad():
        for batch in tqdm.tqdm(data_loader, desc="Evaluating"):
            h = batch['h'].to(device)
            level_h_list = batch['level_h_list'].to(device)
            l_pos = batch['l_pos'].to(device)
            neg_obj = batch['neg_obj'].to(device)
            
            # Forward pass
            h_neg = model(h, level_h_list)
            
            # Normalize features for cosine similarity
            h_neg_norm = h_neg / h_neg.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
            l_pos_norm = l_pos / l_pos.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
            h_norm = h / h.norm(dim=-1, keepdim=True) # [batch_size, embed_dim]
            neg_obj_norm = neg_obj / neg_obj.norm(dim=-1, keepdim=True) # [batch_size, num_objs, embed_dim]
            
            # 1. Similarity alignment loss
            p_sim = F.cosine_similarity(l_pos_norm, h_neg_norm, dim=-1) # [batch_size]
            h_sim = F.cosine_similarity(h_norm, h_neg_norm, dim=-1) # [batch_size]
            neg_sim = F.cosine_similarity(neg_obj_norm, h_neg_norm, dim=-1) # [batch_size, num_objs]
            
            p_sim = torch.mean(p_sim).item()
            h_sim = torch.mean(h_sim).item()
            neg_sim = torch.mean(neg_sim).item()
            
            p_sim_total += p_sim
            h_sim_total += h_sim
            neg_sim_total += neg_sim
    
    batch_count = len(data_loader)
    p_sim_total = p_sim_total / batch_count
    h_sim_total = h_sim_total / batch_count
    neg_sim_total = neg_sim_total / batch_count
    
    print(f"Evaluation results:")
    print(f"  p_sim: {p_sim_total:.4f}")
    print(f"  h_sim: {h_sim_total:.4f}")
    print(f"  neg_sim: {neg_sim_total:.4f}")
    
    sim_loss = (1-neg_sim_total)
    return sim_loss  # è¶Šå°è¶Šå¥½


if __name__ == "__main__":
    # Paths to datasets
    negated_csv = "/root/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv"
    original_csv = "/root/COCO_val_retrieval.csv"
    cfg = {
        # -----æ¨¡å‹å‚æ•°-----
        'dtype': torch.float32,
        'device': 'cuda',
        'num_heads': 4,
        
        'dropout': 0.1,
        'margin': 0.5,
        
        'model_path': os.path.join(current_dir, 'best_clip_lens.pth'), # Lensçš„é¢„è®­ç»ƒæƒé‡
        'save_path': os.path.join(current_dir, 'best_clip_lens.pth'), # Lensçš„è®­ç»ƒæƒé‡ä¿å­˜è·¯å¾„
        
        # -----è®­ç»ƒå‚æ•°-----
        'epochs': 30,
        'batch_size': 32,
        'lr': 1e-3, # full:0.9831 val:0.9765
        # 'lr': 1e-2, # full:0.9857 val:0.9770
        # 'lr': 5e-2, # full:0.9750 val:0.9786
        'weight_decay': 0.05,
        'early_stop_patience': 5, # Early stopping patience
        'test_only': False, # æ˜¯å¦åªæµ‹è¯•
        
        # -----æ•°æ®å‚æ•°-----
        'pos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_retrieval.csv",
        'negpos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv",
        'split': [0.9, 0.1, 0.0],  # train, val, test split
        'num_workers': 4,
        'test_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv",
    }
    
    test_dataset = GlassesDataset(cfg) # Clip_model, lens_model ç”¨äºé¢„åŠ è½½æ•°æ®è¿‡ç¨‹ä¸­çš„ç‰¹å¾æå–
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=cfg['num_workers'])
    
    # if not cfg['test_only']:
    #     model = CLIPGlassesLens(cfg)
    #     # Train model
    #     trained_model = train(cfg, model)
    #     # æµ‹è¯•æ¨¡å‹ä¸ºè®­ç»ƒçš„æ¨¡å‹
    #     cfg['model_path'] = cfg['save_path']
    
    trained_model = CLIPGlassesLens.load_model(cfg)
    trained_model = trained_model.to('cuda')
    
    print("\nFinal evaluation on test set:")
    evaluate(cfg, trained_model, test_loader)
    