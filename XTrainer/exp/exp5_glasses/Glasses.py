import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
grandparent_dir = os.path.dirname(parent_dir)
sys.path.insert(0, grandparent_dir)
sys.path.insert(0, parent_dir)
from utils import setup_logger, set_random_seed
setup_logger(os.path.join(current_dir, "log.txt")) # å°†è¾“å‡ºé‡å®šå‘åˆ°log.txtæ–‡ä»¶
set_random_seed(3407)  # è®¾ç½®éšæœºç§å­
from Lens import CLIPGlassesLens
from Frame import CLIPGlassesFrame
from NegDetector import NegationDetector
from McqDataset import McqDataset, evaluate_model_mcq
from RetrievalDataset_gtneg import RetrievalNegGtDataset, evaluate_model_retrieval_withGTNeg
from RetrievalDataset import RetrievalDataset, evaluate_model_retrieval, retrieval_collate_fn
from CCNegDataset_gtneg import CCNegGtDataset, evaluate_model_CCNeg_etrieval_withGTNeg
from CLSDataset import CLSDataset, evaluate_model_CLS
import torch.nn as nn
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.optim as optim
from torch.nn import functional as F


class Glasses(nn.Module):
    def __init__(self, cfg):
        super().__init__() 
        self.device = cfg['device']
        self.lens = CLIPGlassesLens.load_model(cfg['Lens'])
        self.frame = CLIPGlassesFrame.load_model(cfg['Frame'])
        self.negDetector = NegationDetector.load_model(cfg['NegationDetector']) # è½»é‡çº§å¦å®šåˆ†ç±»å™¨ | 1:åŒ…å«å¦å®š 0:è‚¯å®š
        self.neg_thr = cfg['NegationDetector']['neg_thr'] # å¦å®šé˜ˆå€¼
        self.dtype = cfg['dtype']
       
        # å†»ç»“negDetector
        for param in self.negDetector.parameters():
            param.requires_grad = False
    
    def forward(self, I, h, level_h_list, l_neg=None):
        """
        å‚æ•°:
            - I: å›¾åƒç‰¹å¾ [N_imgs=B, D]
            - h: æœ€åä¸€å±‚ç‰¹å¾ [N_caps=B*num_options, D]
            - level_h_list: å„å±‚ç‰¹å¾åˆ—è¡¨ [N_caps=B*num_options, L, D]
            - l_neg: è¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾ [N_caps=B*num_options, D] | å½“ä¸ºNoneæ—¶ï¼Œä½¿ç”¨lensé¢„æµ‹
        è¿”å›:
            - scores_T2I: æ–‡æœ¬->å›¾åƒçš„åˆ†æ•° [N_caps, N_imgs=B]
            - scores_I2T: å›¾åƒ->æ–‡æœ¬çš„åˆ†æ•° [N_imgs=B, N_caps]
        """
        # å¦å®šæ£€æµ‹
        with torch.no_grad():
            neg_mask = self.negDetector(h).squeeze(-1) > self.neg_thr # å¦å®šé˜ˆå€¼
        
        # Lens        
        if l_neg is None:
            h_neg = self.lens(h, level_h_list)
        else:
            h_neg = l_neg # æµ‹è¯•ç›´æ¥ä½¿ç”¨GTçš„h_neg
        assert I.size(0) == h_neg.size(0) == h.size(0), f"frameè¦æ±‚å›¾ç‰‡åº”è¯¥å’Œæ–‡æœ¬ä¸€å¯¹ä¸€å¯¹åº”"
        
        # Frame
        # scores_T2I = self.frame(I, h, h_neg)
        scores_T2I = self.frame(I, h, h_neg, neg_mask=neg_mask) # å¢åŠ äº†neg_mask
        scores_I2T = scores_T2I.T
        
        return scores_T2I, scores_I2T
    
    def calc_losses(self, scores_T2I, scores_I2T, caption_to_img):
        caption_to_img = torch.tensor(caption_to_img, device=self.device, dtype=torch.long)
        # Textâ†’Image contrastive loss -> å¯ç®€åŒ–ä¸ºCrossEntropyLoss
        loss_txt2img = F.cross_entropy(scores_T2I, caption_to_img)
        # Imageâ†’Text contrastive loss -> ç”±äºä¸€ä¸ªå›¾ç‰‡å¯èƒ½å¯¹åº”å¤šä¸ª captionï¼Œå› æ­¤éœ€è¦å¯¹æ¯ä¸ªå›¾åƒçš„æ‰€æœ‰ caption ç‰¹å¾è¿›è¡Œ softmax
        exp_sim = scores_T2I.exp() # [N_caps, B]
        all_exp = exp_sim.sum(dim=0) # [B]
        # mask[c, i] = 1 -> caption c å±äºå›¾ i
        mask = torch.zeros_like(exp_sim) # [N_caps, B]
        mask[torch.arange(exp_sim.size(0), device=self.device), caption_to_img] = 1
        pos_exp = (exp_sim * mask).sum(dim=0) # [B] # æ­£æ ·æœ¬å¯¹åº”çš„logits
        loss_img2txt = - (pos_exp / all_exp).log().mean() # softmax
        contrastive_loss = 0.5*(loss_txt2img + loss_img2txt)
        total_loss = contrastive_loss
        return total_loss, {'contrastive_loss': contrastive_loss.item()}
    
    @staticmethod
    def load_model(cfg):
        """
        åŠ è½½æ¨¡å‹
        å‚æ•°:
            - cfg: é…ç½®æ–‡ä»¶
            - model_path: æ¨¡å‹è·¯å¾„
        è¿”å›:
            - model: åŠ è½½çš„æ¨¡å‹
        """
        model = Glasses(cfg)
        
        # å¯¼å…¥NegationDetectorçš„æƒé‡
        print(f"æ­£åœ¨åŠ è½½ NegationDetector æ¨¡å‹æƒé‡: {cfg['NegationDetector']['model_path']}")
        model.negDetector.load_state_dict(torch.load(cfg['NegationDetector']['model_path'], weights_only=True))
        
        # å¯¼å…¥Lenså’ŒFrameçš„æƒé‡
        if 'pretrain' in cfg.keys() and cfg['pretrain'] and cfg['model_path'] is not None:
            print(f"è®­ç»ƒï¼šæ­£åœ¨åŠ è½½é¢„è®­ç»ƒ Glasses æ¨¡å‹æƒé‡: {cfg['model_path']}, å°†è¦†ç›– Lens å’Œ Frame çš„æƒé‡ï¼Œä¸è¦†ç›– NegationDetector çš„æƒé‡")
            full_ckpt = torch.load(os.path.join(current_dir, cfg['model_path']), map_location='cpu', weights_only=False)
            filtered_ckpt = {k: v for k, v in full_ckpt.items() if not k.startswith("negDetector.")}
            model.load_state_dict(filtered_ckpt, strict=False)
        if 'test' in cfg.keys() and cfg['test'] is True and cfg['model_path'] is not None:
            print(f"æµ‹è¯•ï¼šæ­£åœ¨åŠ è½½è¢«æµ‹è¯• Glasses æ¨¡å‹æƒé‡: {cfg['model_path']}, å°†è¦†ç›– Lens å’Œ Frame çš„æƒé‡ï¼Œä¸è¦†ç›– NegationDetector çš„æƒé‡")
            full_ckpt = torch.load(os.path.join(current_dir, cfg['model_path']), map_location='cpu', weights_only=False)
            filtered_ckpt = {k: v for k, v in full_ckpt.items() if not k.startswith("negDetector.")}
            model.load_state_dict(filtered_ckpt, strict=False)
        
        return model


def train_Retrieval_with_gtneg(cfg, model:Glasses, with_gt_neg=True):   
    """
    è®­ç»ƒGlassesæ¨¡å‹ | ä»£ç†ä»»åŠ¡: Retrieval with gtneg
    
    å‚æ•°:
        - cfg: é…ç½®æ–‡ä»¶
    """
    # è¯»å–é…ç½®
    device = cfg['device']
    epochs = cfg['epochs']
    clip_grad = True if cfg.get('clip_grad', False) else False # å¦‚æœcfgä¸­æ²¡æœ‰clip_gradï¼Œåˆ™é»˜è®¤ä¸è£å‰ª
    batch_size = cfg['batch_size']
    early_stop_patience = cfg['early_stop_patience'] # Early stopping patience
    lr = cfg['lr']
    num_workers = cfg['num_workers']
    train_rate, val_rate, test_rate = cfg['RetrievalWithGtNeg']['split']

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = RetrievalNegGtDataset(cfg['RetrievalWithGtNeg'])
    print(f">>> train_rate, val_rate, test_rate: {train_rate}, {val_rate}, {test_rate}")
    train_size = int(len(dataset) * train_rate)
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

    # ä¼˜åŒ–å™¨
    if cfg.get('only_train_moudle', None) == 'lens':
        print("åªè®­ç»ƒlensæ¨¡å‹")
        for param in model.frame.parameters():
            param.requires_grad = False
        optimizer = optim.AdamW(model.lens.parameters(), lr=lr, betas=(0.9, 0.98))
    elif cfg.get('only_train_moudle', None) == 'frame':
        print("åªè®­ç»ƒframeæ¨¡å‹")
        for param in model.lens.parameters():
            param.requires_grad = False
        optimizer = optim.AdamW(model.frame.parameters(), lr=lr, betas=(0.9, 0.98))
    else: # è®­ç»ƒæ‰€æœ‰æ¨¡å—
        print("è®­ç»ƒGlassesæ‰€æœ‰æ¨¡å—")
        optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98))
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    # æ¢¯åº¦ç›‘æ§é’©å­
    print("æ³¨å†Œæ¢¯åº¦ç›‘æ§é’©å­")
    for name, param in model.named_parameters():
        if param.requires_grad:
            param.register_hook(
                lambda grad, name=name: print(f"æ¢¯åº¦ {name} èŒƒæ•°: {grad.norm().item():.4f}")
                if grad.norm() > 5e2 else None
            )
    
    # è®­ç»ƒå‰æµ‹è¯•
    evaluate_model_retrieval_withGTNeg(model, val_loader, test_raw_clip=False, with_gt_neg=with_gt_neg)
    
    # Training loop
    best_recall5 = 0
    patience_counter = 0
    for epoch in range(epochs):
        
        model.train()
        epoch_loss = 0
        losses = {'contrastive_loss': 0}
              
        # éå†æ¯ä¸€ä¸ªbatch
        for batch in tqdm(train_loader, desc=f"Epoch{epoch+1}/{epochs}"):
            h = batch['h'].to(device) # CLIPæ–‡æœ¬ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºæ–‡æœ¬ç‰¹å¾(EOSç‰¹å¾) [batch_size, embed_dim]
            level_h = batch['level_h_list'].to(device) # [batch_size, num_layers, embed_dim] CLIPæ–‡æœ¬ç¼–ç å™¨æ¯ä¸€å±‚çš„EOSç‰¹å¾
            l_pos = batch['l_pos'].to(device) # è‚¯å®šæ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
            l_neg = batch['neg_obj'].to(device) # è¢«å¦å®šå¯¹è±¡çš„æ–‡æœ¬ç‰¹å¾ [batch_size, embed_dim]
            I = batch['I'].to(device) # å›¾åƒç‰¹å¾ [batch_size, embed_dim]
            image_ids = batch['img_id'].to(device) # å›¾åƒID [batch_size]
            
            unique_img_ids, remapped_ids = torch.unique(image_ids, sorted=True, return_inverse=True)
            caption_to_img = remapped_ids.cpu().numpy()
            
            # Forward pass
            if with_gt_neg is True:
                scores_T2I, scores_I2T = model(I, h, level_h, l_neg) # ä½¿ç”¨GTçš„h_neg
            else:
                scores_T2I, scores_I2T = model(I, h, level_h) # ä½¿ç”¨lensé¢„æµ‹çš„h_neg
            
            # å°† scores_T2I æ ¹æ® caption_to_img ä» [N_caps, N_imgs] è¿˜åŸä¸º [N_caps, N_imgs]
            cti = torch.tensor(caption_to_img, dtype=torch.long, device=device)  # [N_caps]
            unique_vals = torch.unique(cti, sorted=True)
            first_idx = []
            for val in unique_vals:
                idx = (cti == val).nonzero(as_tuple=True)[0][0]
                first_idx.append(idx)
            first_idx = torch.stack(first_idx, dim=0)  # [N_imgs]
            scores_T2I = scores_T2I[:, first_idx]  # [N_caps, N_imgs]
            scores_I2T = scores_T2I.t()
            
            # Compute loss
            loss, loss_dict = model.calc_losses(scores_T2I, scores_I2T, caption_to_img)
            epoch_loss += loss.item()
            losses['contrastive_loss'] += loss_dict['contrastive_loss']
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if clip_grad:
                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
        
        batch_count = len(train_loader)
        print(f"Ep{epoch+1}/{epochs}  Loss: {epoch_loss/batch_count:.4f} contrastive_loss: {losses['contrastive_loss']/batch_count:.4f}")
        scheduler.step()    
        
        # validation
        val_recall5 = evaluate_model_retrieval_withGTNeg(model, val_loader, test_raw_clip=False, with_gt_neg=with_gt_neg)['mean'][5] # mean-recall@5 
        
        # Save best model
        if val_recall5 > best_recall5:
            best_recall5 = val_recall5
            patience_counter = 0
            torch.save(model.state_dict(), os.path.join(current_dir, cfg['save_path']))
            print(f"Best model saved at epoch {epoch} with recall@5: {best_recall5}")
        else: # æ—©åœ
            patience_counter += 1 # å¢åŠ è€å¿ƒè®¡æ•°å™¨
            print(f"ğŸ’”recall5 drop from {best_recall5:.4f} to {val_recall5:.4f}, cur patience_counter add to {patience_counter}")
            if early_stop_patience > 0 and patience_counter >= early_stop_patience:
                print(f"Early stopping after {epoch+1} epochs")
                break    
        # Save checkpoint
        if epoch % 5 == 0 or epoch == epochs - 1:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': epoch_loss,
                'recall5': val_recall5
            }
            torch.save(checkpoint, os.path.join(current_dir, f"checkpoint_epoch_{epoch}.pth"))
        
        print(f"Training completed. Best validation recall5: {best_recall5:.4f}")
    
    return model

if __name__ == "__main__":
    # Example usagerue
    cfg = {
        # -----è®­ç»ƒå‚æ•°-----
        'epochs': 30,
        'batch_size': 64,
        # 'lr': 5e-3, # 57.47%
        'lr': 1e-5, # r@5: 57.73%
        # 'lr': 1e-4, # r@5: 58.82%
        # 'lr': 10, # r@5: 57.91% - 36.37%
        # 'lr': 1e-5, # r@5: 57.33
        'num_workers': 4,
        'early_stop_patience': 5, # Early stopping patience
        'device': 'cuda',
        'dtype': torch.float32,
        'save_path': 'best_clip_Glasses.pth', # è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹æƒé‡ä¿å­˜è·¯å¾„
        'pretrain': False, # æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒGlasses
        
        # -----æ¨¡å‹å‚æ•°-----
        'Lens': {
            'device': 'cuda',
            'dtype': torch.float32,
            'num_heads': 4,
            'dropout': 0.1,
            'model_path': '/root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/v1/best_clip_lens_9922.pth' # Lensçš„é¢„è®­ç»ƒæƒé‡
        },
        'Frame': {
            'device': 'cuda',
            'dtype': torch.float32,
            'lambda_0': 1, # åŸºç¡€æƒ©ç½šå¼ºåº¦
            # 'model_path': '/root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_clip_Frame_mse_v1869.pth' # Frameçš„é¢„è®­ç»ƒæƒé‡
            # 'model_path': '/root/NP-CLIP/XTrainer/exp/exp5_glasses/best_clip_Frame.pth' # Frameçš„é¢„è®­ç»ƒæƒé‡
        },
        'NegationDetector': {
            'device': 'cuda',
            'model_path': '/root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_NegDet_9404_9212.pth', # NegationDetectorçš„é¢„è®­ç»ƒæƒé‡
            'neg_thr': 0.5, # å¦å®šé˜ˆå€¼(å¤§äºè¯¥å€¼åˆ™ä¸ºå¦å®š) ä¾‹å¦‚ï¼šå…¨å¦å®š: -1.0, å…¨è‚¯å®š: 1.0
        },
        
        # -----æ•°æ®å‚æ•°-----
        'Mcq': {
            'batch_size': 64,
            'num_workers': 4,
            'num_options': 4,
            'split': [0.9, 0.1, 0.0],
            'train_dataset_path': '/root/NP-CLIP/NegBench/data/images/MCQ/COCO_val_mcq_llama3.1_rephrased.csv',
            # 'test_dataset_path': '/root/NP-CLIP/NegBench/data/images/MCQ/COCO_val_mcq_llama3.1_rephrased.csv', # 35.90%
            'test_dataset_path': '/root/NP-CLIP/NegBench/data/images/MCQ/VOC2007_mcq_llama3.1_rephrased.csv',  # 41.66%
        },
        'Retrieval': {
            'batch_size': 64,
            'num_workers': 4,
            'split': [0.9, 0.1, 0.0],
            'train_dataset_path': '/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv',
            'test_dataset_path': '/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv',
        },
        'RetrievalWithGtNeg': { # h_negç›´æ¥ä½œä¸ºGTç»™å‡ºï¼Œåªè®­ç»ƒå’Œæµ‹è¯•Frame
            'batch_size': 64,
            'num_workers': 4, 
            'split': [0.9, 0.1, 0.0],  # train, val, test split
            'pos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_retrieval.csv",
            'negpos_csv_path': "/root/NP-CLIP/NegBench/data/images/Retrieval/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv",
            'dtype': torch.float32, 
        },
        'CCNegGtDataset': {
            'batch_size': 64,
            'num_workers': 4,
            'split': [0.9, 0.1, 0.0],  # train, val, test split
            'csv_path': '/root/NP-CLIP/NegBench/data/ccneg_converted.csv',
            'dtype': torch.float32, 
            
        },
        'ClsEvalDataset': {
            'csv_path': '/root/NP-CLIP/NegBench/data/CLS_Imagenet/imagenet_train.csv',
            'batch_size': 64,
            'num_workers': 4,
        }
        
    }

    # # ä¸€é˜¶æ®µè®­ç»ƒï¼šä½¿ç”¨gtnegä»£æ›¿lensè¾“å‡ºï¼Œå•ç‹¬è®­ç»ƒFrameæ¨¡å‹ï¼Œä¸è®­ç»ƒLensæ¨¡å‹ -- Recall@5: 99.71%
    # cfg['lr'] = 1e-4
    # cfg['neg_thr'] = -1
    # cfg['epochs'] = 10
    # model = Glasses.load_model(cfg)
    # model = train_Retrieval_with_gtneg(cfg, model, with_gt_neg=True) # ä¸€é˜¶æ®µ: è®­ç»ƒGlassesæ¨¡å‹ | ä»£ç†ä»»åŠ¡: Retrieval with gtneg
    
    # # äºŒé˜¶æ®µè®­ç»ƒï¼šä½¿ç”¨GT_negä½œä¸ºç›‘ç£å•ç‹¬è®­ç»ƒlens, åœ¨lens.pyä¸­å®Œæˆ
    
    # # ä¸‰é˜¶æ®µè®­ç»ƒï¼šè”åˆè®­ç»ƒlenså’ŒFrameæ¨¡å‹ï¼Œè¿›è¡Œé€‚é… -- Recall@5: val: 75.97% full: 82.24%  -- MCQ: 35.90%
    # cfg['pretrain'] = True
    # cfg['lr'] = 1e-3
    # cfg['model_path'] = 'best_clip_Glasses.pth' # ä¸€é˜¶æ®µé¢„æ¨¡å‹æƒé‡è·¯å¾„
    # cfg['neg_thr'] = -1
    # cfg['clip_grad'] = True # æ¢¯åº¦è£å‰ª
    # model = Glasses.load_model(cfg)
    # model.lens = CLIPGlassesLens.load_model(cfg['Lens']) # åŠ è½½lensæ¨¡å‹çš„é¢„è®­ç»ƒæƒé‡
    # model = train_Retrieval_with_gtneg(cfg, model, with_gt_neg=False) # äºŒé˜¶æ®µ: è”åˆlensè®­ç»ƒGlassesæ¨¡å‹ | ä»£ç†ä»»åŠ¡: Retrieval
    
    # æµ‹è¯•æ¨¡å‹é€šç”¨é…ç½®
    cfg['test_raw_clip'] = True
    cfg['test'] = True
    # cfg['model_path'] = 'weights/v1/best_clip_Glasses_7597_8224_3590(after_joint).pth' # æµ‹è¯•æ¨¡å‹æƒé‡è·¯å¾„
    cfg['model_path'] = 'weights/v2/best_clip_Glasses.pth'
    # cfg['model_path'] = 'best_clip_Glasses.pth'
    cfg['Lens']['model_path'], cfg['Frame']['model_path'] = None, None # ä¸è¦†ç›–jointè®­ç»ƒåçš„Glasses
    cfg['NegationDetector']['model_path'] = '/root/NP-CLIP/XTrainer/exp/exp5_glasses/weights/best_NegDet_9404_9212.pth'
    
    # æµ‹è¯•Imagenetä¼ ç»Ÿåˆ†ç±»èƒ½åŠ›ä¿ç•™ç¨‹åº¦
    cfg['ClsEvalDataset']['csv_path'] = '/root/NP-CLIP/NegBench/data/CLS/imagenet_val.csv' # ours:52.40% CLIP:53.87%
    test_dataset = CLSDataset(cfg['ClsEvalDataset'])
    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg['ClsEvalDataset']['batch_size'], shuffle=False, num_workers=cfg['ClsEvalDataset']['num_workers'])
    if cfg['test_raw_clip'] is True:
        evaluate_model_CLS(None, test_dataloader, test_raw_clip=True)
    else:
        model = Glasses.load_model(cfg)
        evaluate_model_CLS(model, test_dataloader, test_raw_clip=False)
    
    # æµ‹è¯•Imagenet1Kä¼ ç»Ÿåˆ†ç±»èƒ½åŠ›ä¿ç•™ç¨‹åº¦
        
    # # æµ‹è¯•caltech101ä¼ ç»Ÿåˆ†ç±»èƒ½åŠ›ä¿ç•™ç¨‹åº¦
    # cfg['ClsEvalDataset']['csv_path'] = '/root/NP-CLIP/NegBench/data/CLS/caltech101.csv' # ours:90.54% clip:90.74%
    # test_dataset = CLSDataset(cfg['ClsEvalDataset'])
    # test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg['ClsEvalDataset']['batch_size'], shuffle=False, num_workers=cfg['ClsEvalDataset']['num_workers'])
    # if cfg['test_raw_clip'] is True:
    #     evaluate_model_CLS(None, test_dataloader, test_raw_clip=True)
    # else:
    #     model = Glasses.load_model(cfg)
    #     evaluate_model_CLS(model, test_dataloader, test_raw_clip=False)
        
    # # æµ‹è¯•CIFAR-100ä¼ ç»Ÿåˆ†ç±»èƒ½åŠ›ä¿ç•™ç¨‹åº¦
    # cfg['ClsEvalDataset']['csv_path'] = '/root/NP-CLIP/NegBench/data/CLS/cifar100.csv' # ours:38.50% clip:37.04%
    # test_dataset = CLSDataset(cfg['ClsEvalDataset'])
    # test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg['ClsEvalDataset']['batch_size'], shuffle=False, num_workers=cfg['ClsEvalDataset']['num_workers'])
    # if cfg['test_raw_clip'] is True:
    #     evaluate_model_CLS(None, test_dataloader, test_raw_clip=True)
    # else:
    #     model = Glasses.load_model(cfg)
    #     evaluate_model_CLS(model, test_dataloader, test_raw_clip=False)
        
    # # æµ‹è¯•CIFAR-10ä¼ ç»Ÿåˆ†ç±»èƒ½åŠ›ä¿ç•™ç¨‹åº¦
    # cfg['ClsEvalDataset']['csv_path'] = '/root/NP-CLIP/NegBench/data/CLS/cifar10.csv' # ours:71.03% clip:71.08%
    # test_dataset = CLSDataset(cfg['ClsEvalDataset'])
    # test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg['ClsEvalDataset']['batch_size'], shuffle=False, num_workers=cfg['ClsEvalDataset']['num_workers'])
    # if cfg['test_raw_clip'] is True:
    #     evaluate_model_CLS(None, test_dataloader, test_raw_clip=True)
    # else:
    #     model = Glasses.load_model(cfg)
    #     evaluate_model_CLS(model, test_dataloader, test_raw_clip=False)
    
    # # æµ‹è¯• CC-Neg
    # test_ccneg_dataset = CCNegGtDataset(cfg['CCNegGtDataset'])
    # test_ccneg_dataloader = torch.utils.data.DataLoader(test_ccneg_dataset, batch_size=cfg['CCNegGtDataset']['batch_size'], shuffle=False, num_workers=cfg['CCNegGtDataset']['num_workers'])
    # if cfg['test_raw_clip'] is True:
    #     evaluate_model_CCNeg_etrieval_withGTNeg(None, test_ccneg_dataloader, test_raw_clip=True, with_gt_neg=False)
    # else:
    #     model = Glasses.load_model(cfg)
    #     evaluate_model_CCNeg_etrieval_withGTNeg(model, test_ccneg_dataloader, test_raw_clip=False, with_gt_neg=False) # ä½¿ç”¨lensé¢„æµ‹çš„h_neg
    
    # # æµ‹è¯• Retrieval with gtbeg
    # test_retrieval_dataset = RetrievalNegGtDataset(cfg['RetrievalWithGtNeg'])
    # test_retrieval_dataloader = torch.utils.data.DataLoader(test_retrieval_dataset, batch_size=cfg['Retrieval']['batch_size'], shuffle=False, num_workers=cfg['Retrieval']['num_workers'])
    # if cfg['test_raw_clip'] is True:
    #     evaluate_model_retrieval_withGTNeg(None, test_retrieval_dataloader, test_raw_clip=True, with_gt_neg=False)
    # else:
    #     model = Glasses.load_model(cfg)
    #     # evaluate_model_retrieval_withGTNeg(model, test_retrieval_dataloader, test_raw_clip=False, with_gt_neg=True) # ä½¿ç”¨GTçš„h_neg
    #     evaluate_model_retrieval_withGTNeg(model, test_retrieval_dataloader, test_raw_clip=False, with_gt_neg=False) # ä½¿ç”¨lensé¢„æµ‹çš„h_neg
        
    # # æµ‹è¯• MCQ VOC 
    # cfg['Mcq']['test_dataset_path'] = '/root/NP-CLIP/NegBench/data/images/MCQ/VOC2007_mcq_llama3.1_rephrased.csv'
    # test_retrieval_dataset = McqDataset(cfg['Mcq']['test_dataset_path'])
    # test_retrieval_dataloader = torch.utils.data.DataLoader(test_retrieval_dataset, batch_size=cfg['Mcq']['batch_size'], shuffle=False, num_workers=cfg['Mcq']['num_workers'])
    # if cfg['test_raw_clip'] is True:
    #     evaluate_model_mcq(None, test_retrieval_dataloader, test_raw_clip=True)
    # else:
    #     model = Glasses.load_model(cfg)
    #     evaluate_model_mcq(model, test_retrieval_dataloader, test_raw_clip=False)
        
    # # æµ‹è¯• MCQ COCO
    # cfg['Mcq']['test_dataset_path'] = '/root/NP-CLIP/NegBench/data/images/MCQ/COCO_val_mcq_llama3.1_rephrased.csv'
    # test_retrieval_dataset = McqDataset(cfg['Mcq']['test_dataset_path'])
    # test_retrieval_dataloader = torch.utils.data.DataLoader(test_retrieval_dataset, batch_size=cfg['Mcq']['batch_size'], shuffle=False, num_workers=cfg['Mcq']['num_workers'])
    # if cfg['test_raw_clip'] is True:
    #     evaluate_model_mcq(None, test_retrieval_dataloader, test_raw_clip=True)
    # else:
    #     model = Glasses.load_model(cfg)
    #     evaluate_model_mcq(model, test_retrieval_dataloader, test_raw_clip=False)
    
    
    print("==============é…ç½®é¡¹===============")
    for k, v in cfg.items():
        if isinstance(v, dict):
            print(f"{k}:")
            for k1, v1 in v.items():
                print(f"  {k1}: {v1}")
        else:
            print(f"{k}: {v}")
    print("===================================")