# CLIP 在下游任务的迁移应用

> BY:  Junhao Xiao (xiaojunhao066@gmail.com)

---

### CLIP4clip（视频-文本检索）

本文将 **CLIP迁移到视频-文本检索任务** 。

![image-20250314160003559](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250314160003559.png)

**关键适配措施**：

* 基于 CLIP 设计**三种相似度计算方法**（视频特征池化方法），**池化掉视频特征的时间维度，以与文本特征对齐**。

  ![image-20250314155951673](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250314155951673.png)

  > * **小数据集:  ** 使用 **均值池化** 减少新参数的引入，防止过拟合
  > * **大数据集：**引入**自注意力**等机制，可以更好地建模时间依赖关系

* **在大规模视频-文本数据集上进一步预训练 CLIP4Clip**，增强模型的检索能力。

---

###  ActionCLIP（视频动作识别）

核心基本和CLIP4clip模型差不多，只是**讲了个故事**，本质是一样的。

> **故事：** 提出了一种新范式 “ 预训练、提示（Prompt）和微调（Fine-tune）” 
>
> * 预训练：直接使用CLIP的预训练权重
> * 提示：文本提示（label套入模板），图像提示（池化掉视频特征的时间维度，以与文本特征对齐）

<img src="C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250228224617197.png" alt="image-20250228224617197" style="zoom: 33%;" />

![image-20250228225614995](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250228225614995.png)

---

### 2022-(clip-语义分割)-DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting

![image-20250310233444769](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250310233444769.png)

本文提出了一种新的语言引导密集预测框架——**DenseCLIP**：受 CLIP 原始对比学习框架的启发，将 CLIP 的“图像-文本匹配”任务转换为“**像素-文本匹配**”任务，利用**像素-文本匹配得分图**来指导模型的学习。

#### 语言引导的密集预测（Language-Guided Dense Prediction）

**1. CLIP 图像编码器结构**

不仅使用 CLIP 提取的**全局图像特征 **$\bar{z}$ ，还可以利用 CLIP **图像编码器的最后一层特征图** $z$ ，并将其转换为 **语言兼容的特征图** ，从而提升**密集预测任务**的表现。

> 在 CLIP 的标准训练过程中，仅使用 $\bar{z}$ 作为图像编码器的最终输出，而 $z$ **通常被忽略**。但本文发现 $z$ 具有两个重要特性：
>
> 1. **保留了空间信息**，可以作为密集预测任务的特征图使用。
> 2. **与语言特征对齐良好**，因为 MHSA（多头注意力） 处理输入时是对称的，因此 $z$ 可能会类似于 $\bar{z}$，从而与文本特征自然匹配。

以 ResNet 作为 CLIP 的图像编码器为例，其包含四个阶段（stages），特征图记作$\{x_i\}_{i=1}^4$。与标准 ResNet 不同，CLIP 进行了一个关键改动：在第四阶段的输出 $x_4$ 之后**添加了一个注意力池化层**。

具体过程如下：

1. **全局平均池化**
   对 $x_4$ 进行全局平均池化，得到全局图像特征：$\bar{x}_4 \in \mathbb{R}^{1 \times C}$ 

2. **多头自注意力（MHSA）计算**

   将$[\bar{x}_4, x_4]$送入多头自注意力层（Multi-Head Self-Attention, MHSA）：
   $$
   [\bar{z}, z] = \text{MHSA}([\bar{x}_4, x_4])
   $$

   >- $\bar{z} \in \mathbb{R}^{1 \times C}$ 是全局特征
   >- $z \in \mathbb{R}^{H_4 W_4 \times C}$ 是注意力处理后的特征图

   对于 ViT 这样的架构，可以通过 **去除 CLS token** 的方式获得类似的特征图。

**2. 像素-文本匹配得分计算**

为了获取文本特征，首先使用 CLIP 的文本编码器对 $K$ 个类别名进行编码，形成文本特征：$t \in \mathbb{R}^{K \times C}$

然后，将**图像特征图 $z$** 和**文本特征 $t$** 进行匹配，计算**像素-文本得分图**：
$$
s = \hat{z} \hat{t}^\top, \quad s \in \mathbb{R}^{H_4 W_4 \times K}
$$

> $\hat{z}, \hat{t}$ 是 $z$ 和 $t$ 在通道维度上进行 $ℓ2$ 归一化后的结果。

像素-文本得分图 $s$ 具有两个重要作用：

1. 作为**低分辨率**的**分割结果**，可以用于计算**辅助分割损失**，帮助模型优化。
2. 作为附加信息，与原始特征 $x_4$ 进行拼接：$x'_4 = [x_4, s] \in \mathbb{R}^{H_4 W_4 \times (C+K)}$ 这样就能**显式地将语言先验融入到视觉特征中**。

这个方法**不依赖于特定的模型结构**，因为修改后的特征图可以**直接用于下游任务**，如目标检测或分割，只需对 FPN 等组件的输入维度进行轻微调整。

**3. 上下文感知的提示学习（Context-Aware Prompting）**

在**视觉或语言域减少 domain gap 能显著提升 CLIP 在下游任务的性能**（例如CoOp）。因此，本文不直接使用预定义的文本模板，而是采用**上下文感知的提示学习**，利用**视觉上下文**来优化**文本特征**。（例如："a photo of a cat in the grass." 比单纯的 "a photo of a cat." 更准确）

<img src="C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250310235311224.png" alt="image-20250310235311224" style="zoom: 33%;" />

**(0) ❌基准：CoOp提示**

采用 **CoOp** 作为基准，仅使用**语言域提示**。输入文本编码器的文本特征为：
$$
[p, e_k], 1 \leq k \leq K
$$

> - $p \in \mathbb{R}^{N \times C}$ 代表可学习的文本上下文。
> - $e_k \in \mathbb{R}^{C}$ 代表第 $k$ 类的类别名称嵌入。

##### **(1) ❌预文本编码提示（Pre-Model Prompting）**

即**在文本编码器前**融合**视觉信息**：
$$
v_{\text{pre}} = \text{TransDecoder}(q, [\bar{z}, z])
$$

>- $q \in \mathbb{R}^{N \times C}$ 是一组可学习的查询
>- $v_{\text{pre}}$ 是提取出的视觉上下文
>- 视觉上下文 **$v_{\text{pre}}$ 替换掉 基准 中的 $p$**，成为文本编码器的输入

**❌缺点**：**推理时需要额外的文本编码器前向计算**，影响效率。

##### **(2) ✔后文本编码提示（Post-Model Prompting）**

即**在文本编码器后**优化文本特征：
$$
v_{\text{post}} = \text{TransDecoder}(t, [\bar{z}, z])
$$
这一实现方式**鼓励文本特征寻找最相关的视觉线索**。随后，我们通过**残差连接更新文本特征**： $t \leftarrow t + \gamma v_{\text{post}}$

>- $v_{\text{post}}$ 是文本特征经过 Transformer 解码器后获得了图像上下文信息的结果。
>- $\gamma \in \mathbb{R}^C$是一个**可学习参数**，用于控制残差的缩放比例。为了最大程度地保留文本特征中的语言先验，$\gamma$ 在初始化时被赋予一个非常小的值（例如 $10^{-4}$）。

✔**后文本编码提示更优**：

1. **计算更高效**：**训练后可以存储优化后的文本特征**，减少推理时的计算量。
2. **性能更强**：实验表明，其效果优于**预文本编码提示**。

---

### 2024-AAAI-(clip-视线估计)CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model

**related work：**依靠**单一的视觉形态**不足以处理所有与凝视无关的因素。

<img src="C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250308205403206.png" alt="image-20250308205403206" style="zoom:33%;" />

我们提出了一种名为CLIP- gaze的新方法，该方法利用预训练的视觉语言模型CLIP**将一般可转移知识传授给凝视估计模型**，并增强提取的凝视特征的**泛化能力**。因此CLIP- gaze可以利用CLIP灵活地**处理各种与凝视无关的因素**，而不是依赖~~昂贵的模型~~或~~不可控的对抗方法~~，只能处理~~有限的~~凝视无关因素。

![image-20250308220645475](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250308220645475.png)

#### **模型组成**

1. **CLIP 模型（固定不训练）**
   1. 生成**图像特征** $f_v$ 。
   2. 生成基于文本模板构造的**凝视无关特征** $\{ f_k^{ir} \}_{k=1}^{K}$。
2. **凝视估计模型（可训练）**
   - 让凝视特征提取器$E$ **蒸馏CLIP模型的图像编码模块的知识**，并通过 MLP从$f_{re}$中过滤出**凝视相关特征** $f_{re}$，通过对比学习让$f_{re}$**远离所有凝视无关特征** $\{ f_k^{ir} \}_{k=1}^{K}$，最终得到更鲁棒的凝视特征 $f_{re}$，从而提升泛化能力。

#### **CLIP-Gaze 的工作流程**

**1. 生成眼动无关特征（Gaze-irrelevant Features）**

- CLIP-Gaze 采用 CLIP 文本编码器（$C^t$）基于预定义的 **文本提示模板（prompt template）**$\text{"An image of a face with }\{c_k\}\text{"}$ 来构造 **眼动无关特征**$\{ f_k^{ir} \}_{k=1}^{K}$

**1.2 眼动特征蒸馏（Distill to CLIP Feature Space）**

对于 **输入图像 $x$**，眼动估计过程可以表示为：$\hat{g} = F(M(E(x)))$

- $E(x)$：ResNet-18 提取的原始图像特征 $f$（包含 **眼动相关** 和 **无关** 的信息）。
- $M(f)$：MLP 作为 **特征过滤器**，分离出 **眼动相关特征 $f^{re}$**。
- $F(f^{re})$：全连接层（FC）用于最终预测眼动方向 $\hat{g}$。

为了**与CLIP 视觉编码器 $C^v$ 对齐**（蒸馏其知识），CLIP-Gaze 采用如下损失函数：
$$
L_d (f, f_v) = 1 - \frac{f \cdot f_v}{\|f\| \|f_v\|} + 1 \times 0.5
$$

> - $f^v$ 是 **CLIP 视觉编码器** $C^v$ 提取的特征。
> - 该损失 **衡量 $f$ 与 $f^v$ 在 CLIP 特征空间的对齐程度**，并归一化到 `[0,1]`。

**1.3 眼动相关特征分离（Separate Gaze-relevant Features）**

为了训练 $M(f)$ 仅提取 **眼动相关特征** $f^{re}$，CLIP-Gaze 需要**最小化** $f^{re}$ 和 **眼动无关特征** $f^{ir}_k$ 的相似度：

$$
L_{ir} (f^{re}, f^{ir}_k) = \sum_{k=1}^{K} \tilde{w_k} \frac{f^{re} \cdot f^{ir}_k}{\|f^{re}\| \|f^{ir}_k\|}
$$

> - 每个样本的 **无关因素不同**，因此使用 **权重 $w_k$** 来调整 $f^{re}$ 与 $f^{ir}_k$ 的**相似度**：$w_k = \frac{f \cdot f^{ir}_k}{\|f\| \|f^{ir}_k\|}$
> - 归一化后：$\tilde{w_k} = \text{softmax}(w_1, ..., w_K)$ 这样可以**降低无关因素对 $f^{re}$ 的干扰**，使得 **模型仅关注与眼动相关的特征**。

**1.4 眼动方向预测（Gaze Estimation）**

在得到 $f^{re}$ 之后，CLIP-Gaze 通过**全连接层** $F(f^{re})$ 预测最终的 **眼动方向 $\hat{g}$**，并使用如下 **$L_g$损失** ，即**角度误差**作为优化目标：

$$
L_g (\hat{g}, g) = \arccos \left( \frac{\hat{g} \cdot g}{\|\hat{g}\| \|g\|} \right)
$$

> - $\hat{g}$ 是模型预测的眼动方向。
> - $g$ 是真实眼动方向的 **ground-truth** 标签。
> - (因为$arccos(cos(\theta))=\theta$ 计算的是**两个向量之间的夹角**)

该损失函数可以确保 $\hat{g}$ **尽可能接近 $g$ 的方向**，从而提高眼动估计的准确性。

#### 个性化上下文优化（Personalized Context Optimization, PCO）

对于预训练的V-L模型，文本输入（即prompt）在下游数据集中起着关键作用。然而，确定正确的提示是一项困难的任务，通常需要花大量的时间进行单词调优——措辞上的细微变化可能会对性能产生巨大的影响。

**先前工作:**

* 由于**CoOp**这种**固定的提示无法适应每个个体的个性化特征**，而**CoCoOp** 通过轻量级神经网络生成与图像内容相关的提示，但直接利用图像内容可能引入**与凝视相关的信息**，从而影响模型提取凝视无关特征的能力。**PCO** 提出了一种新颖的方法，专门设计了基于**人脸身份特征**的个性化提示生成策略。PCO利用 **3DMM** **提取个体的身份特征**，并通过 **Meta-Net** 生成**个性化的文本提示**，使其仅捕捉**凝视无关特征**（如身份、表情、性别等）。

![image-20250309132121791](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250309132121791.png)

PCO 在 CLIP 特征空间中最大化图像特征与匹配的文本特征之间的相似度。定义类别预测概率和相应的训练损失函数为：
$$
p_k = \frac{\exp(\text{sim}(f_v, f_t^k) / \tau)}{\exp(\text{sim}(f_v, f_t^k) / \tau) + \exp(\text{sim}(f_v, f_t^{k^-}) / \tau)}，L_p = -\sum_{k=1}^{K}
$$

> - $f_v$ 是通过 CLIP 图像编码器提取的图像特征；
> - $f_t^k$ 是正类别的文本特征；
> - $f_t^{k^-}$ 是反类别的文本特征（通过否定词生成，如“not happy”对应“happy”）；

在训练完成后，PCO 用于生成最终的**凝视无关特征**，帮助MLP学到如何从原始Clip图像编码器得到的图像特征中**过滤掉这些凝视无关特征**，即让**过滤后的图像特征**和**凝视无关特征**直接的**距离尽可能的远**。

#### 基于排序的凝视相关特征优化

**关于对比回归损失CRLoss: ** Wang等人（2022）提出了一种对比回归损失**CRLoss** ，其设定一个**阈值**，使得**具有较大角度差异的凝视特征远离**，而**具有较小角度差异的凝视特征靠近**。然而，CRLoss 具有两个主要问题：

* **阈值难以确定**：不同数据集或应用场景可能需要不同的阈值。
* **忽略凝视特征的细粒度关系**：凝视估计是一个连续回归任务，而 CRLoss 仅关注是否超出阈值，而非特征间的顺序关系。

我们不设定固定的阈值，而是让模型**自动学习凝视特征之间的相对关系**，让模型学会区分哪些凝视方向比较接近，哪些不同，并且在提取特征时自动遵循这种规律，提高凝视估计的准确性。

**1. 计算两个相似度**

对于一对样本 $(i, j)$，我们计算：

- **凝视方向相似度** $s_g$：$s_g^{ij} = \frac{g_i \cdot g_j}{\|g_i\| \|g_j\|}$，其中 $g_i$ 和 $g_j$ 是两个样本的凝视方向（即 gaze labels）。
- **特征相似度** $s_f$：$s_f^{ij} = \frac{f_{re}^i \cdot f_{re}^j}{\|f_{re}^i\| \|f_{re}^j\|}$，其中 $f_{re}$ 是模型提取的 gaze-relevant 特征。

**2. 强制特征相似度的排序和凝视相似度一致**

有了这些相似度后，我们做如下操作：

- 选取两对样本 **(pair 1: $(i, j)$)** 和 **(pair 2: $(m,n)$)**。

- 比较它们的凝视相似度：

  - 如果 $s_g^{ij} > s_g^{mn}$，说明第一对的 gaze label 更相似。
  - 我们希望模型学到：$s_f^{ij}$ 也应该比 $s_f^{mn}$ 大（即，**特征相似度应该和gaze相似度呈正相关**）。

- 设计一个 **排序损失（Rank Loss）** 来强制学习这个顺序：
  $$
  L_{re} = \max(0, -S_{12} \cdot (s_f^{1} - s_f^{2}))
  $$

  > - $S_{12}$ 是一个指示值：
  >   - 如果 $s_g^{1} > s_g^{2}$，则 $S_{12} = 1$（应该保持相对顺序）。
  >   - 如果 $s_g^{1} < s_g^{2}$，则 $S_{12} = -1$（应该交换顺序）。
  > - 只有当 $s_f^{1} - s_f^{2}$ 的顺序与 $S_{12}$ 不匹配时，损失才会大于 0，从而对模型施加惩罚，迫使模型学到正确的排序关系。

**3. 训练过程**

- 在一个 mini-batch 里，我们构造 $O = \frac{B(B-1)}{2}$ 组样本对。
- 每次随机选两对样本，计算 **Rank Loss** 并进行梯度更新。

#### 总损失函数

我们的模型优化目标是一个加权的损失函数，由四个部分组成：
$$
L_g + \lambda_1 L_d + \lambda_2 L_{ir} + \lambda_3 L_{re}
$$

> 1. $L_g$ 直接**优化 gaze 估计回归任务**，使 gaze 预测尽可能准确。
> 2. $L_d$ 通过监督模型**蒸馏Clip图像编码器**的知识（图像特征提取方式）（**全盘接收**）。$L_{ir}$ 使模型**剥离掉与 gaze 估计中不相关的特征干扰**，提高鲁棒性（**去其糟粕**）。
> 3. $L_{re}$ 通过让模型**强制学习`特征相似度的排序应该符合真实的gaze相似度的排序` 这个子任务**，促使模型**侧重学习gaze识别方面的特征提取**。

-----

### 2024-AAAI-叶茫-An Empirical Study of CLIP for Text-Based Person Search(Clip-基于文本的人物搜索)

<img src="C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250309185652250.png" alt="image-20250309185652250" style="zoom:33%;" />

对 CLIP 在文本-行人检索（Text-Based Person Search，TBPS）任务中的应用进行了深入的实验分析，主要从 **数据增强** 和 **损失函数** 两个方面入手，并最终提出了一种高效、轻量级的 **TBPS-CLIP** 方案。

![image-20250309214107514](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250309214107514.png)

-----

#### 1. 图文对比损失（Image-Text Contrastive Loss, ITC）

CLIP 等模型采用 **图文对比损失（ITC）** 进行图像-文本匹配，核心思想是：**正样本（匹配的图像-文本对）靠近**， **负样本（不匹配的图像-文本对）远离**

ITC 计算图像 $I_i$ 与文本 $T_j$ 的相似度，并通过 softmax 归一化得到**匹配概率**：
$$
p_{i,j} = \frac{\exp(f_{I_i} \cdot f_{T_j} / \tau)}{\sum_{k=1}^{N} \exp(f_{I_i} \cdot f_{T_k} / \tau)}
$$

> - $f_{I_i}$ 和 $f_{T_j}$ 是 $\ell_2$ 归一化的特征向量
> - $\tau$ 是可学习的温度参数
> - $p_{i,j}$ 表示图像 $I_i$ 与文本 $T_j$ 匹配的概率
> - 由于Clip进行了L2归一化，因此余弦相似度$s_f^{ij} = \frac{f_{T_i} \cdot f_{T_j}}{\|f_{T_i}\| \|f_{T_j}\|}$可以简化为$f_{T_i} \cdot f_{T_j}$

ITC 采用**交叉熵损失**进行优化：$L_{ITC} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{N} q_{i,j} \log p_{i,j}$，其中 $q_{i,j}$ 是真实匹配标签（1 表示匹配，0 表示不匹配），鼓励**正确的匹配**。

#### 2. 归一化图文对比损失（Normalized Image-Text Contrastive Loss, N-ITC）

**问题**：ITC **直接使用二元标签（1 或 0）**进行优化，可能导致优化过程不够稳定。N-ITC 通过 **归一化标签** 解决此问题。

归一化后的标签分布定义为：$\hat{q}_{i,j} = \frac{q_{i,j}}{\sum_{k=1}^{N} q_{i,k}}$

N-ITC 损失函数为：$L_{N-ITC} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{N} \hat{q}_{i,j} \log p_{i,j}$

#### 3. 反向图文对比损失（Reversed Image-Text Contrastive Loss, R-ITC）

**问题**：N-ITC 主要优化 $D_{KL}(Q \parallel P)$，即让 $P$ 逼近 $Q$，但它**更关注正样本匹配（正样本的损失主导），而忽略了负样本的优化**。

**解决方案**：R-ITC 采用 **$D_{KL}(P \parallel Q)$ 方向的优化**，即**更关注负样本的分离**。

其损失函数定义为：$L_{R-ITC} = \frac{1}{2} \left( D_{KL}(P \parallel \hat{Q}) + D_{KL}(P^T \parallel \hat{Q}^T) \right)$

展开形式如下：$L_{R-ITC} = \frac{1}{2N} \sum_{i=1}^{N} \sum_{j=1}^{N} \left( p_{i,j} \log \frac{p_{i,j}}{\hat{q}_{i,j} + \epsilon} + p_{j,i} \log \frac{p_{j,i}}{\hat{q}_{j,i} + \epsilon} \right)$

#### 4. 循环图文对比损失（Cyclic Image-Text Contrastive Loss, C-ITC）

> 一种正则化损失

**问题**：N-ITC 和 R-ITC 关注图文的对比学习，但可能会导致**表征空间的不规则分布**，影响检索性能。

**解决方案**：C-ITC 引入**几何一致性约束**，**优化表征空间的几何结构**，减少错误检索。

- **几何一致性**：保证特征空间的结构稳定性
- **减少错误检索**：降低文本查询时匹配到错误图像的风险

C-ITC 由两个部分组成：

##### 4.1 模态内正则化（In-Modality Regularization, CI-ITC）

该损失保证**同一模态的样本间相似度在不同模态中保持一致**，定义如下：$L_{CI-ITC} = \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{N} (\text{sim}(I_i, I_j) - \text{sim}(T_i, T_j))^2$

##### 4.2 交叉模态正则化（Cross-Modality Regularization, CC-ITC）

该损失保证**不同模态的匹配关系在跨模态中保持一致**，定义如下：$L_{CC-ITC} = \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{N} (\text{sim}(I_i, T_j) - \text{sim}(I_j, T_i))^2$

##### 4.3 总体 C-ITC 公式

$$
L_{C-ITC} = L_{CI-ITC} + L_{CC-ITC}
$$

----

在对比学习中，**Self-Supervision（SS，自监督）** 和 **Multi-View Supervision（MVS，多视角监督）** 是两种不同的监督信号来源。

#### **5. Self-Supervision（自监督）**

**核心思想**：利用数据本身的信息来生成伪标签，从而避免对人工标注的依赖。常见的自监督方式有**基于数据增强的对比学习**，即通过对同一张图片进行不同的变换（如旋转、裁剪、颜色抖动等）来生成不同视角的表示，并期望模型学习到这些变换后的特征仍然属于同一类。

#### **6. Multi-View Supervision（多视角监督）**

**核心思想**：通过不同的视角或模式的信息来提供监督信号，以增强对比学习的效果。这里的“多视角”可以指：

1. **不同模态（Multi-Modal）**：如图像-文本（CLIP）、RGB-深度图等。
2. **不同传感器（Multi-Sensor）**：如LIDAR-相机、多角度相机等。
3. **不同增强方式（Augmentation View）**：从不同的增强策略得到多个视角，并通过一致性学习来提升泛化能力。

---

### 2025-ICASSP-CR-CLIP - (文本描述+对比学习)for gaze预测

<img src="C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250308132505729.png" alt="image-20250308132505729" style="zoom:33%;" />

一种**基于 CLIP 的通用 gaze 估计方法**，通过**文本描述+对比学习+加权角度误差**来改进 gaze 预测。

`A person is looking to [direction] [angle]’, where [direction] is a set of base directions (up, down, left, right), and [angle] is the gaze direction angle after rounding.`

#### **将 gaze 方向从连续数值转为文本描述：**

- 直接预测 gaze 方向的角度是一个**回归任务**，但作者**改用文本描述**，让 CLIP 处理 gaze 估计。

- 预定义 gaze 方向的文本格式：**"A person is looking to [方向] [角度]"**
  - **`[方向]` 取值：**up（上）、down（下）、left（左）、right（右）。
  - **`[角度]` 取值：**对 gaze 方向角度进行四舍五入得到的值。

#### **在pitch和yaw两个特征空间中分别进行对比学习：**

**文本编码器：**

- 为了减少文本表示学习的难度，把 gaze 方向拆分成**pitch 和 yaw**，分别学习。

- **Pitch（俯仰角）文本编码**：$P^{\text{pitch}}_i = \text{TextEncoder}(\text{tokenize}(p^{\text{pitch}}_i))$

- **Yaw（偏航角）文本编码**：$P^{\text{yaw}}_i = \text{TextEncoder}(\text{tokenize}(p^{\text{yaw}}_i))$

  > 其中，$p^{\text{pitch}}_i$ 和 $p^{\text{yaw}}_i$ 分别是 **pitch 和 yaw 角度的文本描述**。

**图像编码器：**

* 使用 **CLIP 预训练的 ResNet-50** 作为图像编码器，从输入的人脸图像 $x_i$ 中提取 gaze 特征。
* 使用 **两个 MLP（多层感知机）** 将图像特征投影到 **pitch 和 yaw 空间**
  * $X^{\text{pitch}}_i = \text{MLP}^{\text{pitch}}(\text{ImageEncoder}(x_i))$，对应 pitch（俯仰角）的图像特征
  * $X^{\text{yaw}}_i = \text{MLP}^{\text{yaw}}(\text{ImageEncoder}(x_i))$，对应 yaw（偏航角）的图像特征

#### **损失函数：**

**★ 图像-文本对比学习损失（Contrastive Loss）**

使正确的图像-文本对的相似度最大化，而错误配对的相似度最小化。

* **计算相似度（Cosine Similarity）**：
  $$
  \text{sim}_{ij}^{yaw} = \frac{X_i^{yaw} \cdot P_j^{yaw}}{\|X_i^{yaw} \|\|P_j^{yaw} \|}
  $$

  > 其中：$X_i^{yaw}$ 是第 $i$张图像的特征向量，$P_j^{yaw}$ 是第 $j$ 个文本的特征向量，计算的是**余弦相似度**，即两个向量的点积除以它们的模长

- **图像到文本的损失**：
  $$
  L_{\text{img2text}}^{yaw} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\text{sim}_{ii}^{yaw} / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}_{ij}^{yaw} / \tau)}
  $$

  >- 目标：让正确匹配的图像-文本对（对角线上的 ${sim}_{ii}^{yaw}$）的相似度更大，相比于其他错误匹配的 $\text{sim}_{ij}^{yaw}$。
  >- $\tau$ 是温度参数，用于调整分布的锐度

- **文本到图像的损失**：
  $$
  L_{\text{text2img}}^{yaw} = (L_{\text{img2text}}^{yaw})^T
  $$

  > * 这个损失是对称的，表示从文本到图像的对比学习损失。

- **最终的对比学习分类损失（Total Contrastive Loss）**：
  $$
  L_{\text{cls}}^{yaw} = \frac{1}{2} (L_{\text{text2img}}^{yaw} + L_{\text{img2text}}^{yaw})
  $$

  > * 这个损失同时优化从图像到文本和从文本到图像的匹配。

**★ Top-k 对比回归损失（Top-k Contrastive Regression Loss）**

传统对比损失只关注匹配关系，而不关心**数值预测的精确度**，这在**凝视估计（gaze estimation）**等回归任务中是不够的。这个回归损失考虑了**预测结果和真实值之间的角度误差**，并**根据预测置信度进行加权**。
$$
L_{\text{reg}}^{yaw} = \frac{1}{B} \sum_{i=1}^{B} \sum_{j=1}^{k} \left( |g_i^{yaw} - g_j^{yaw}| \times \frac{\exp(\text{sim}_{ij})}{\sum_{j=1}^{k} \exp(\text{sim}_{ij})} \right)
$$

> * $g_i^{yaw}$ 是第 $i$ 个样本的**真实凝视方向**（ground truth）。$g_j^{yaw}$ 是**Top-k 预测的 gaze 角度**。$\text{sim}_{ij}$ 是第 $i$ 张图像与第 $j$ 个 gaze 预测文本的**相似度**。$B$：批量大小（batch size）。$k$：超参数，设定为 $B$$/2$（即 batch size 一半）。
>
> * **Top-k 选择**：选取最相似的 $k$ 个 gaze 文本预测，并使用 softmax 权重它们的贡献。
>
> **直观理解**：
>
> 1. 计算预测的 gaze 角度与真实 gaze 角度之间的绝对角度误差。
> 2. 对误差加权，权重由 softmax 计算出的概率分布决定（更相似的预测权重更高）。

**★ 总损失（Total Loss）**
$$
L = L_{\text{cls}}^{yaw} + L_{\text{cls}}^{pitch} + \lambda L_{\text{reg}}^{yaw} + \lambda L_{\text{reg}}^{pitch}
$$


> * $L_{\text{cls}}^{yaw}$ 和 $L_{\text{cls}}^{pitch}$ 代表**对比学习的分类损失**，分别用于**偏航角（yaw）**和**俯仰角（pitch）**。
>
> - $L_{\text{reg}}^{yaw}$ 和 $L_{\text{reg}}^{pitch}$ 代表**回归损失**，用于提高预测的数值精度。
> - $\lambda$ 是超参数，控制分类损失和回归损失的平衡，**实验中设置 $\lambda = 1.0$**。

#### **训练+微调**

★ **训练**

训练按照上述进行训练。

★ **微调**
$$
\hat{y}_i = \text{MLP}_{\text{reg}}(\text{AttnPool}(\text{ImageEncoder}_{L-1}(x_i)))
$$

* 舍弃掉 **~~文本编码器~~** 和 ~~**图像编码器的最后一层（输出层）**~~，**只保留图像编码器的特征提取层**（在训练阶段联合文本编码器训练好的，仍然保留丰富的视觉特征，但未完全对齐文本空间，因此更适合 gaze 估计等回归任务）。

* 使用 **Attention Pooling（注意力池化）**用于**降低特征维度**，CLIP 图像编码器的中间层特征可能仍然是 **高维特征图**，直接输入 MLP 计算量较大，且可能导致过拟合。过注意力机制池化，可以**提取最相关的信息**。

* 降维后的特征输入到 **多层感知机（MLP）** 进行 **gaze 方向回归**。

* 使用 **L1 Loss** 作为微调阶段的损失函数。计算真实 gaze 方向 $g$ 和预测 gaze 方向 $\hat{g}$ 之间的 **绝对误差**。比 L2 损失（均方误差）**更鲁棒（抗离群点能力更强）**，适用于 gaze 估计这种可能有一定误差的任务。
  $$
  L_{\text{gaze}} = \| g - \hat{g} \|_1Lgaze=∥g−g^∥1
  $$

---

==*-----------------------------结合音频的多模态------------------------------------*==

---

### 2024-Audio-Visual CLIP-音视频Clip-STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft

> #### ⛳⛳⛳ **启发**
>
> - ⛳如果只对**音频编码器**训练一个转换网络，使其**适应视频编码器的潜在空间**，可能会**限制音频嵌入的表达能力**，因为它==**被强行约束去“模仿”**==视频编码器的特征表示空间。
> - ⛳通过**同时训练音频和视频的转换网络**，模型可以==**自由调整两个模态的潜在空间**==，并**在一个新的共享隐空间 (shared latent space) 进行对齐**，从而提高整体的泛化能力和跨模态对齐能力。

我们提出了一个**Audio-Visual CLIP基础模型**，该模型包含一个**冻结的视频编码器**和一个**冻结的音频编码器**，以及在这些编码器之上**训练的非线性变换网络**，以将编码器的嵌入映射到一个**共享的潜在空间**（shared latent space）。这样一来，模型可以学习到**音频和视频之间的对齐关系**，实现跨模态的理解和检索。

<img src="C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250313233653467.png" alt="image-20250313233653467" style="zoom: 33%;" />

> 该模型是在我们提出的**音视频数据集 (Audio-Video dataset)** 上进行训练的

#### **视频编码器（Video Encoder）**

> 预训练的MineCLIP主要用于适配《我的世界》的像素图像，此处我们可以使用**传统的CLIP的视觉编码器**即可。

- 我们的音视频 CLIP 模型使用 **MineCLIP <u>预训练</u>视频编码器 [6]** ，该编码器能够将 **16 帧的视频和文本共同映射到一个嵌入空间**。
- 其内部采用**基于 16 帧的时序池化 (temporal pooling) 机制**：
  1. **帧级别嵌入**：先对输入的 **16 帧视频** 逐帧进行特征提取，得到 **16 个帧级嵌入**。
  2. **时序池化**：将这 **16 个帧级别的嵌入合并**，最终形成一个**全局视频嵌入**。

这种方法能够**有效捕捉视频的时序特征**，同时通过**文本引导的视频编码方式**，使得模型能够理解**视频内容与文本描述之间的对应关系**。

#### **音频编码器 (Audio Encoder)**

音频数据采用 **预训练的 Audio Spectrogram Transformer (AST) [32] 作为音频编码器**：

1. **音频转换为频谱图**：先将原始音频信号转换为 **时频谱表示**，类似于图像，提供**时序上的频率变化信息**。
2. **特征提取**：将这个 **频谱输入 AST 编码器**（Audio Spectrogram Transformer），该模型最初是用于<u>音频分类任务</u>，例如识别语音、环境声音、交通噪音等。
3. **生成音频嵌入（Audio Embeddings）**：最终，我们从 AST 的输出层获取**音频特征表示（logits）**，并将其作为**音频嵌入**输入给**转换网络**。

#### **转换网络 (Transformation Networks)**

为了让音频和视频都能映射到 **相同的共享潜在空间 (shared latent space)**，研究者训练了两个 **非线性转换网络** 来调整编码器的输出。

**(1) 为什么需要转换网络？**

由于视频和音频编码器的**嵌入向量维度不同**：

- **视频编码器（MineCLIP）输出维度为 512**
- **音频编码器（AST）的输出维度为 527**

这两个特征向量**维度不匹配**，因此需要一个映射网络来 **调整维度**，确保它们能够被映射到相同的**共享潜在空间 (shared latent space)**。

**(2) 训练两个转换网络的原因**

作者强调了一个关键设计决策：对 **音频 和 视频编码器 分别 都训练一个转换网络**，而~~不仅仅是训练音频转换网络~~。原因如下：

- ⛳如果只对**音频编码器**训练一个转换网络，使其**适应视频编码器的潜在空间**，可能会**限制音频嵌入的表达能力**，因为它==**被强行约束去“模仿”**==视频编码器的特征表示空间。
- ⛳通过**同时训练音频和视频的转换网络**，模型可以==**自由调整两个模态的潜在空间**==，并**在一个新的共享隐空间 (shared latent space) 进行对齐**，从而提高整体的泛化能力和跨模态对齐能力。

**(3) 转换网络基于 StyleGAN 3**

转换网络的架构是 **StyleGAN 3 的映射网络的变体**：

- StyleGAN 3 映射网络的主要功能是**将输入（如噪声向量）投影**到一个**高维潜在空间**，使得**特征更加解耦**，提高生成能力。
- 在 **Audio-Video CLIP** 中，**转换网络**的作用是**将音频和视频编码器的输出投影到同一个潜在空间**，以便于音视频之间的对齐和比较。

**具体调整**

- **层数增加**：原本 StyleGAN 3 的映射网络有 **8 层 MLP**，而 **这里增加到了 10 层**，以增强转换能力。
- **隐藏层维度增加**：从 **512 维扩展到 1024 维**，提供**更大的学习空间**，提高对音视频特征的表达能力。
- **使用 余弦相似度 作为相似性度量**：这一点借鉴了原始 CLIP 模型的思路，即通过最大化**匹配的音视频特征之间的余弦相似度**，来训练转换网络，使得音视频数据**在共享隐空间中尽可能对齐**。

#### 训练音视频CLIP基础模型

我们的训练方案**与原始CLIP模型的训练方法相同**[5]，<u>唯一的区别是我们没有使用余弦调度器来衰减学习率</u>。如前所述，我们在训练过程中**冻结了两个编码器模型的权重**，**只更新变换网络的权重**，并将变换网络训练了**100个epochs**。

---

### 2023-AAAI-RUC-视频+音频模态融合-CLIP4VLA

音频预训练模型 能够有效表示复杂的音频信息。然而，直接从零开始学习视觉、文本和音频之间的通用关联需要极高的计算成本。因此，一个直接的想法是**将最先进的VL模型与预训练的音频骨干网络结合起来**。

#### **两个主要挑战：**

1. **异构模型架构问题**
   文本、视觉和音频的**预训练模型**通常具有**不同的网络结构**，这使得**采用统一的训练策略进行融合变得困难**。

   > 例如，用于自动语音识别（ASR）的音频预训练模型通常在**音素级别**处理音频，其**参数规模远大于VL模型**，导致**直接结合变得不现实**。 

2. **音频信息的多样性问题**
   目前还**没有一个单一的音频骨干网络能够全面处理**一般音频中的**不同类型的信息**，这些信息可以大致分为**言语信息** 和 **非言语信息**。

   由于这两类信息的**异质性**，现有的**音频模型通常仅专注于处理其中之一**。然而，要实现对视频的全面理解，两类信息都不可或缺。

#### 相关工作：

**音频预训练**

* 在**非语言信息**的编码方面，近期研究证明了**音频表示学习**可以通过**迁移学习**从**其他模态（如图像）**中受益。
* 在**语言信息**的编码方面，**自监督学习**方法被广泛应用于学习音频的内在特性，主要包括：**自回归学习**，**对比学习**。
* **OURS:** 以往的研究仅关注**某一种类型**的音频，而 CLIP4VLA 同时提取**语言信息**和**非语言信息**，以实现对**通用视频**的理解。

**三模态理解**

* **OPT** 主要关注**图片描述中的语音**，这与通用视频的音频信息存在较大差异。

* **VATT**  研究了**用统一的模态无关编码器**来表示视觉、文本和音频三种模态。

  > VATT **从零开始训练，训练成本极高**，且并**未区分音频中的语言信息和非语言信息。**
  >
  > CLIP4VLA **基于现有的视觉-语言（VL）预训练模型**进行**三模态关联学习**，大幅降低了训练开销，并能同时提取**语言信息**和**非语言信息**。

#### CLIP4VLA模型结构（Vision, Language and Audio）

为了克服上述两个挑战，我们提出了**CLIP4VLA（CLIP for Vision, Language and Audio）**，该模型通过**统一的三编码器**结构扩展CLIP，以适应音频模态并实现高效的多模态处理。

![ ](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250314003128552.png)

**> 数据提取**

我们首先**从视频中提取音频**，并将**视频、音频和文本**批次分别表示为 $V$、$A$ 和 $T$。CLIP4VLA 旨在学习这三种模态的丰富语义表示，使得语义相似的视频、音频和文本被嵌入到相近的表示空间中，而语义不同的样本则保持较远的距离。

**> 文本 & 视觉编码器 (Text & Vision Encoder)**

我们采用 **预训练的CLIP 作为文本和视觉编码器**，以分别对文本输入 $T$ 和视觉输入 $V$ 进行编码。具体来说：

- **文本编码：**每个文本 $t_i \in T$ 先被分词，并添加**起始标记** `[SOS]` 和**结束标记** `[EOS]`，记为 ${ t^1_i, t^2_i, \dots, t^{L_T}_i }$。编码后，每个标记的输出表示为词级别嵌入 ${ x^1_i, x^2_i, \dots, x^{L_T}_i }$。我们遵循 CLIP 的做法，选择 `[EOS]` **标记**的输出作为文本的**全局表示** $x^g_i$。
- **视觉编码：**对于每个视频 $v_i \in V$，我们在时间维度上**均匀采样** $L_V$ 帧，形成视觉序列 ${ v^1_i, v^2_i, \dots, v^{L_V}_i }$。每一帧被分割为**不重叠的 patch**，并添加 **`[CLS]` 标记**。视觉编码器独立处理每一帧的 patch 序列，以建模空间关系。最终，我们选取 **`[CLS]` 标记的输出**作为**每帧的视觉表示**，记为 $y_i = \{ y^1_i, y^2_i, \dots, y^{L_V}_i \}$。通过对 $y_i$ 进行**平均池化**，得到**全局视觉嵌入** $y^g_i$。

**> 音频编码器 (Audio Encoder)**

![image-20250314003200661](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250314003200661.png)

训练有素的专家可以通过**观察声谱图**推测**环境事件**或**人类语音**。因此，机器也可以通过**视觉声谱图**的输入来**编码音频信息**。为了保证不同模态的架构一致性，我们设计的**音频编码器与视觉编码器结构相同**。

- **音频预处理**：
  首先将 **1D 音频信号** $a_i \in A$ 转换为**图像格式**，即大小为 $224×224×3$ 的矩阵。具体步骤如下：
  1. 将**音频波形**转换为 **224 维对数 Mel 滤波器组（fbank）特征**，每 8ms 计算一次，窗口大小 32ms。这样，一个 **$t$ 秒长的音频流**可转换为**形状为 $125t×224$ 的声谱图**。
  2. **沿时间维度将声谱图裁剪**为多个 $224×224$ 的patch（不重叠），若最后一部分小于 224，则用零填充。
  3. 最终，$t$ 秒的音频被转换为 $\lceil t / 1.792 \rceil$ 帧，每帧大小为 $R^{224 \times 224 \times 3}$。
- **音频编码**： 经过上述转换后，我们得到音频帧序列 ${ a^1_i, a^2_i, \dots, a^{L_A}_i }$，可采用与**视觉编码类似的方法进行处理**。最终的音频段表示序列为 $z_i = { z^1_i, z^2_i, \dots, z^{L_A}_i }$。通过对 $z_i$ 进行平均池化，得到全局音频嵌入 $z^g_i$。

**> 音频类型标记 (Audio Type Token)**

为有效**控制音频编码器提取哪种类型的特征**（语言信息，非语言信息），我们设计了 **音频类型标记**。

在展平音频片段的 patch 序列后，根据不同应用场景，在序列末尾添加 `[VB]` 或 `[NB]` 标记，以**灵活提取语言及非语言信息**。

- 对话或解说类音频使用 `[VB]` 提取**语言信息**；
- 自然事件或环境音使用 `[NB]` 提取**非语言信息**；
- **既包含语言信息又包含非语言信息**的复杂场景可以结合**两种标记**，以更全面地理解视频内容。

![image-20250314145806139](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250314145806139.png)

#### CLIP4VLA 预训练和微调方法

**> 预训练**

CLIP4VLA 通过 **跨模态对比学习** 和 **模态内自监督学习** 进行**预训练**，以优化文本、视觉和音频的**联合表示能力**。

- **跨模态对比学习**

  跨模态学习的目的是**对齐文本、视觉和音频模态**，使语义相似的样本在不同模态之间的嵌入更接近，而语义不同的样本嵌入距离更远。具体做法如下：

  * 在一个批量大小 $B$ 的**批次**（minibatch）内，计算文本、视觉和音频的全局嵌入（特征），构造 **文本-音频** 和 **视觉-音频** 正负样本对。计算 $B \times B$ 维度的余弦相似度矩阵。

  * 通过最大化正确匹配（正样本对）的余弦相似度，最小化错误匹配（负样本对）的余弦相似度，训练音频编码器。模型的**对比损失**（NCE Loss）如下：
    $$
    NCE_{at} = \frac{1}{B} \sum_{i}^{B} \log \frac{\exp(z^g_i \cdot x^g_i)}{\sum_{j}^{B} \exp(z^g_i \cdot x^g_j)}
    $$

    $$
    NCE_{av} = \frac{1}{B} \sum_{i}^{B} \log \frac{\exp(z^g_i \cdot y^g_i)}{\sum_{j}^{B} \exp(z^g_i \cdot y^g_j)}
    $$

    > $x^g$、$y^g$、$z^g$ 分别表示**文本**、**视觉**和**音频**的全局嵌入


- **模态内对比学习**

  为**增强音频编码器的表示能力**，模型采用 **自监督学习** 进行**模态内优化**：

  * 对音频数据进行**掩码增强**：

    - 在 **通道维度** 和 **时间维度上** 随机选择 5% 和 15% 的位置进行**遮挡**，每次遮挡 10 个连续步长。

    - **允许不同遮挡区域之间重叠**，以增加数据的多样性。

  * 通过模态内**对比学习**，优化音频的自监督表示，使**原始音频**与其**增强版本**形成**正样本对**，提高模型的**鲁棒性**。

**> 微调下游任务**

预训练完成后，CLIP4VLA **在多个下游任务上进行微调**，以验证其在文本、视觉和音频表示学习上的有效性。

* **视频检索**

  视频检索任务旨在基于文本查询找到对应的视频。 CLIP4VLA 充分利用 **文本、视觉和音频** 三种模态信息，具体提供了**三种不同的融合策略**：

  * **全局对全局**：对文本、视觉和音频的**全局嵌入 进行 平均池化**，然后**计算相似度**。
  * **全局对局部**：引入 **视频时间编码模块**（N 层 Transformer），捕捉**视频帧 和 音频片段 **的**时间相关性**，与 **文本特征** 计算**相似度**。
  * **局部对局部**：引入 **细粒度跨模态融合模块**（N 层 Transformer），**深入挖掘** 文本与视觉、音频 的 **局部特征相关性**。

  ![image-20250314150429647](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250314150429647.png)

  ![image-20250314145941820](C:\Users\xiaoj\AppData\Roaming\Typora\typora-user-images\image-20250314145941820.png)

* **视频字幕生成**

  视频字幕生成任务的目标是根据视频内容生成流畅的自然语言描述。CLIP4VLA 通过 **多模态字幕生成器**（MCG） 进行字幕生成。

  在第 $t$ 个解码步骤：

  - 输入：之前生成的 **文本，视频帧，音频片段**

  - 经过 CLIP4VLA 的**编码器**处理，生成**多模态特征序列** $U_i$，通过MCG处理序列： $H_i = MCG(U_i)$。

  - **计算词级别概率分布**：$p^t_i = \text{softmax}(f(h^t_i)), h^t_i \in H_i$ 其中 $f(\cdot)$ 是**线性输出层**，$p^t_i$ 是**当前时间步生成单词的概率分布**。

---

